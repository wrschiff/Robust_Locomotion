{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar  6 17:56:16 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A4000               On  |   00000000:65:00.0  On |                  Off |\n",
      "| 41%   45C    P8             17W /  140W |      89MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA RTX A4000               On  |   00000000:B3:00.0 Off |                  Off |\n",
      "| 41%   39C    P8             15W /  140W |      15MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A   1337515      G   /usr/lib/xorg/Xorg                             60MiB |\n",
      "|    0   N/A  N/A   1337552      G   /usr/bin/gnome-shell                            7MiB |\n",
      "|    1   N/A  N/A   1337515      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "Environment variable MUJOCO_GL set to: egl\n",
      "Checking that the installation succeeded:\n",
      "Installation successful.\n",
      "XLA_FLAGS set to:  --xla_gpu_triton_gemm_any=True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import distutils.util\n",
    "\n",
    "# Check for GPU (remove or modify if you're running on a CPU-only machine)\n",
    "if subprocess.run('nvidia-smi', shell=True).returncode:\n",
    "    raise RuntimeError(\n",
    "        'Cannot communicate with GPU. '\n",
    "        'Make sure you have an NVIDIA GPU with the proper drivers installed.')\n",
    "\n",
    "# Add an ICD config so that glvnd can pick up the Nvidia EGL driver.\n",
    "NVIDIA_ICD_CONFIG_PATH = '/usr/share/glvnd/egl_vendor.d/10_nvidia.json'\n",
    "if not os.path.exists(NVIDIA_ICD_CONFIG_PATH):\n",
    "    with open(NVIDIA_ICD_CONFIG_PATH, 'w') as f:\n",
    "        f.write(\"\"\"{\n",
    "    \"file_format_version\" : \"1.0.0\",\n",
    "    \"ICD\" : {\n",
    "        \"library_path\" : \"libEGL_nvidia.so.0\"\n",
    "    }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "# Set environment variable to use GPU rendering:\n",
    "os.environ['MUJOCO_GL'] = 'egl'\n",
    "print('Environment variable MUJOCO_GL set to:', os.environ['MUJOCO_GL'])\n",
    "\n",
    "try:\n",
    "    print('Checking that the installation succeeded:')\n",
    "    import mujoco\n",
    "    mujoco.MjModel.from_xml_string('<mujoco/>')\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        'Something went wrong during installation. Check the terminal output for more information.\\n'\n",
    "        'If using a hosted runtime, make sure GPU acceleration is enabled.'\n",
    "    ) from e\n",
    "\n",
    "print('Installation successful.')\n",
    "\n",
    "# Tell XLA to use Triton GEMM for improved performance (if applicable)\n",
    "xla_flags = os.environ.get('XLA_FLAGS', '')\n",
    "xla_flags += ' --xla_gpu_triton_gemm_any=True'\n",
    "os.environ['XLA_FLAGS'] = xla_flags\n",
    "print('XLA_FLAGS set to:', os.environ['XLA_FLAGS'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All packages imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# If you haven't installed mediapy yet:\n",
    "# %pip install mediapy\n",
    "\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "from typing import Callable, NamedTuple, Optional, Union, List\n",
    "\n",
    "# Instead of the apt-install commands, just ensure ffmpeg is installed in your environment.\n",
    "\n",
    "import mediapy as media\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# More legible printing from numpy.\n",
    "np.set_printoptions(precision=3, suppress=True, linewidth=100)\n",
    "\n",
    "#@title Import MuJoCo, MJX, and Brax\n",
    "from datetime import datetime\n",
    "from etils import epath\n",
    "import functools\n",
    "from IPython.display import HTML\n",
    "from typing import Any, Dict, Sequence, Tuple, Union\n",
    "import os\n",
    "from ml_collections import config_dict\n",
    "\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jp\n",
    "import numpy as np\n",
    "from flax.training import orbax_utils\n",
    "from flax import struct\n",
    "from matplotlib import pyplot as plt\n",
    "import mediapy as media\n",
    "from orbax import checkpoint as ocp\n",
    "\n",
    "import mujoco\n",
    "from mujoco import mjx\n",
    "\n",
    "from brax import base\n",
    "from brax import envs\n",
    "from brax import math\n",
    "from brax.base import Base, Motion, Transform\n",
    "from brax.base import State as PipelineState\n",
    "from brax.envs.base import Env, PipelineEnv, State\n",
    "from brax.mjx.base import State as MjxState\n",
    "from brax.training.agents.ppo import train as ppo\n",
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "from brax.io import html, mjcf, model\n",
    "\n",
    "\n",
    "print(\"All packages imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want the humanoid with the trey, uncomment the rewards accordingly, also change the xml path. (note: left arm reward might be messed up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Humanoid Env\n",
    "#from pathlib import path\n",
    "import jax.numpy as jnp    \n",
    "\n",
    "#HUMANOID_ROOT_PATH = epath.Path(epath.resource_path('mujoco')) / 'mjx/test_data/humanoid'\n",
    "HUMANOID_ROOT_PATH = epath.Path(\"/home/wrschiff/Desktop/mujoco/Robust_Locomotion/\")\n",
    "\n",
    "class Humanoid(PipelineEnv):\n",
    "\n",
    "   def __init__(\n",
    "      self,\n",
    "      forward_reward_weight=1.25,\n",
    "      ctrl_cost_weight=0.1,\n",
    "      healthy_reward=5.0,\n",
    "      terminate_when_unhealthy=True,\n",
    "      healthy_z_range=(1.0, 2.0),\n",
    "      ########### WEIGHT FOR BALANCING TRAY ############\n",
    "      #balance_tray_reward_weight=0, # setting this to 0 for now\n",
    "      #terminate_when_boxfall=True,\n",
    "      ##################################################\n",
    "      #left_arm_pose_penalty_weight=0, #also setting this to 0 for initial policy\n",
    "      reset_noise_scale=1e-2,\n",
    "      exclude_current_positions_from_observation=True,\n",
    "      **kwargs,\n",
    "   ):\n",
    "   #\n",
    "      mj_model = mujoco.MjModel.from_xml_path(os.getcwd() + '/basic_humanoid_no_tray.xml')\n",
    "      mj_model.opt.solver = mujoco.mjtSolver.mjSOL_CG\n",
    "      mj_model.opt.iterations = 6\n",
    "      mj_model.opt.ls_iterations = 6\n",
    "\n",
    "      sys = mjcf.load_model(mj_model)\n",
    "\n",
    "      physics_steps_per_control_step = 5\n",
    "      kwargs['n_frames'] = kwargs.get(\n",
    "         'n_frames', physics_steps_per_control_step)\n",
    "      kwargs['backend'] = 'mjx'\n",
    "\n",
    "      super().__init__(sys, **kwargs)\n",
    "\n",
    "      self._forward_reward_weight = forward_reward_weight\n",
    "      self._ctrl_cost_weight = ctrl_cost_weight\n",
    "      self._healthy_reward = healthy_reward\n",
    "      self._terminate_when_unhealthy = terminate_when_unhealthy\n",
    "      self._healthy_z_range = healthy_z_range\n",
    "      self._reset_noise_scale = reset_noise_scale\n",
    "      ########### WEIGHT FOR BALANCING TRAY ############\n",
    "      #self._balance_tray_reward_weight = balance_tray_reward_weight\n",
    "      #self._terminate_when_boxfall = terminate_when_boxfall\n",
    "      ##################################################\n",
    "      self._exclude_current_positions_from_observation = (\n",
    "         exclude_current_positions_from_observation\n",
    "      )\n",
    "\n",
    "      ########### INDICES FOR BOX AND TRAY IN XPOS ARRAY ############\n",
    "      #self.tray_x_id = mujoco.mj_name2id(self.sys.mj_model, mujoco.mjtObj.mjOBJ_XBODY, \"tray\")\n",
    "      #self.box_x_id = mujoco.mj_name2id(self.sys.mj_model, mujoco.mjtObj.mjOBJ_XBODY, \"box\")\n",
    "      ###############################################################\n",
    "\n",
    "      #self._left_arm_pose_penalty_weight = left_arm_pose_penalty_weight\n",
    "\n",
    "      #self.target_left_arm_pose = jax.numpy.array([0.0, 0.0, 0.0])\n",
    "\n",
    "      # Identify left arm joint indices (adjust joint names as needed)\n",
    "      self.left_arm_joint_ids = jnp.array([\n",
    "         mujoco.mj_name2id(self.sys.mj_model, mujoco.mjtObj.mjOBJ_JOINT, \"shoulder1_left\"),\n",
    "         mujoco.mj_name2id(self.sys.mj_model, mujoco.mjtObj.mjOBJ_JOINT, \"shoulder2_left\"),\n",
    "         mujoco.mj_name2id(self.sys.mj_model, mujoco.mjtObj.mjOBJ_JOINT, \"elbow_left\")\n",
    "      ])\n",
    "\n",
    "\n",
    "   def reset(self, rng: jp.ndarray) -> State:\n",
    "      \"\"\"Resets the environment to an initial state.\"\"\"\n",
    "      rng, rng1, rng2 = jax.random.split(rng, 3)\n",
    "\n",
    "      low, hi = -self._reset_noise_scale, self._reset_noise_scale\n",
    "      qpos = self.sys.qpos0 + jax.random.uniform(\n",
    "         rng1, (self.sys.nq,), minval=low, maxval=hi\n",
    "      )\n",
    "      qvel = jax.random.uniform(\n",
    "         rng2, (self.sys.nv,), minval=low, maxval=hi\n",
    "      )\n",
    "\n",
    "      data = self.pipeline_init(qpos, qvel)\n",
    "\n",
    "      \n",
    "      obs = self._get_obs(data, jp.zeros(self.sys.nu))\n",
    "      reward, done, zero = jp.zeros(3)\n",
    "      metrics = {\n",
    "         'forward_reward': zero,\n",
    "         'reward_linvel': zero,\n",
    "         'reward_quadctrl': zero,\n",
    "         'reward_alive': zero,\n",
    "         'x_position': zero,\n",
    "         'y_position': zero,\n",
    "         'distance_from_origin': zero,\n",
    "         'x_velocity': zero,\n",
    "         'y_velocity': zero,\n",
    "      }\n",
    "      return State(data, obs, reward, done, metrics)\n",
    "   def step(self, state: State, action: jp.ndarray) -> State:\n",
    "      \"\"\"Runs one timestep of the environment's dynamics.\"\"\"\n",
    "      data0 = state.pipeline_state\n",
    "      data = self.pipeline_step(data0, action)\n",
    "\n",
    "      com_before = data0.subtree_com[1]\n",
    "      com_after = data.subtree_com[1]\n",
    "      velocity = (com_after - com_before) / self.dt\n",
    "      forward_reward = self._forward_reward_weight * velocity[0]\n",
    "\n",
    "      min_z, max_z = self._healthy_z_range\n",
    "      is_healthy = jp.where(data.q[2] < min_z, 0.0, 1.0)\n",
    "      is_healthy = jp.where(data.q[2] > max_z, 0.0, is_healthy)\n",
    "      if self._terminate_when_unhealthy:\n",
    "         healthy_reward = self._healthy_reward\n",
    "      else:\n",
    "         healthy_reward = self._healthy_reward * is_healthy\n",
    "\n",
    "      ctrl_cost = self._ctrl_cost_weight * jp.sum(jp.square(action))\n",
    "\n",
    "      ############## CALCULATE BOX-TRAY REWARD ##################\n",
    "      #euclid_dist_tb = jp.linalg.norm(data.x.pos[self.tray_x_id] - data.x.pos[self.box_x_id])\n",
    "      #balance_cost = euclid_dist_tb * self._balance_tray_reward_weight\n",
    "      ###########################################################\n",
    "\n",
    "      obs = self._get_obs(data, action)\n",
    "      ############## ADD TO OVERALL REWARD ##################\n",
    "      reward = forward_reward + healthy_reward - ctrl_cost #- balance_cost\n",
    "      #######################################################\n",
    "\n",
    "      reward = forward_reward + healthy_reward - ctrl_cost #- balance_cost\n",
    "\n",
    "      # Compute left arm steadiness penalty\n",
    "      #left_arm_joint_angles = data.q[self.left_arm_joint_ids]      \n",
    "      \n",
    "      # Compute pose deviation error\n",
    "      #pose_error = left_arm_joint_angles - self.target_left_arm_pose\n",
    "      #pose_penalty = self._left_arm_pose_penalty_weight * jp.sum(jp.square(pose_error))\n",
    "        \n",
    "       # Subtract the penalty from the total reward\n",
    "      #reward = reward - pose_penalty\n",
    "\n",
    "      #print(f'CTRL COST BEFORE SCALAR (as benchmark): {ctrl_cost}')\n",
    "      #print(f'EUCLID DISTANCE: {euclid_dist_tb} \\t\\tSCALED REWARD: {balance_cost}\\t\\tTOTAL REWARD: {reward}')\n",
    "\n",
    "      ########## ADDING TERMINATION CONSTRAINT IF BOX FALLS OFF TRAY ############\n",
    "      #is_balanced = data.x.pos[self.tray_x_id][2] < data.x.pos[self.box_x_id][2]\n",
    "      #done = 0.0\n",
    "      #done = jp.where(self._terminate_when_unhealthy, 1.0 - is_healthy, 0.0)\n",
    "      #done = jp.where(\n",
    "      #   (self._terminate_when_boxfall) & (done == 0.0),  # both must be True\n",
    "      #   1.0 - is_balanced,\n",
    "      #   done\n",
    "      #)\n",
    "\n",
    "      ###########################################################################\n",
    "      # PREVIOUS METHOD: done = 1.0 - is_healthy if self._terminate_when_unhealthy else 0.0\n",
    "\n",
    "      #print(f'TRAY HEIGHT: {data.x.pos[self.tray_x_id][2]}\\tBOX HEIGHT: {data.x.pos[self.box_x_id][2]}\\tDONE:{done}')\n",
    "      \n",
    "      state.metrics.update(\n",
    "         forward_reward=forward_reward,\n",
    "         reward_linvel=forward_reward,\n",
    "         reward_quadctrl=-ctrl_cost,\n",
    "         reward_alive=healthy_reward,\n",
    "         x_position=com_after[0],\n",
    "         y_position=com_after[1],\n",
    "         distance_from_origin=jp.linalg.norm(com_after),\n",
    "         x_velocity=velocity[0],\n",
    "         y_velocity=velocity[1],\n",
    "      )\n",
    "\n",
    "      return state.replace(\n",
    "         pipeline_state=data, obs=obs, reward=reward, done=done\n",
    "      )\n",
    "\n",
    "   def _get_obs(\n",
    "      self, data: mjx.Data, action: jp.ndarray\n",
    "   ) -> jp.ndarray:\n",
    "      \"\"\"Observes humanoid body position, velocities, and angles.\"\"\"\n",
    "      position = data.qpos\n",
    "      if self._exclude_current_positions_from_observation:\n",
    "         position = position[2:]\n",
    "\n",
    "      # external_contact_forces are excluded\n",
    "      return jp.concatenate([\n",
    "         position,\n",
    "         data.qvel,\n",
    "         data.cinert[1:].ravel(),\n",
    "         data.cvel[1:].ravel(),\n",
    "         data.qfrc_actuator,\n",
    "      ])\n",
    "envs.register_environment('humanoid', Humanoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the environment\n",
    "env_name = 'humanoid'\n",
    "env = envs.get_environment(env_name)\n",
    "\n",
    "# define the jit reset/step functions\n",
    "jit_reset = jax.jit(env.reset)\n",
    "jit_step = jax.jit(env.step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This example is still simplified – for instance, the minibatch selection does not do random shuffling. You can adapt it to your own needs (e.g., randomizing minibatch order). Also, it’s jitted only partially. For a full, production‐grade Brax PPO, you may want to pmap or vmap certain computations. But this script should demonstrate the correct logic for truncated BPTT, advantage handling, and KL early stopping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALSO: The Reward Cycler hasn't been fully implemented yet. Follow the reward cycle from either of these papers: https://arxiv.org/pdf/2204.04340 or https://arxiv.org/pdf/2011.01387 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 336\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m params\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 336\u001b[0m     trained_params \u001b[38;5;241m=\u001b[39m train_recurrent_ppo(\n\u001b[1;32m    337\u001b[0m         env_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhumanoid\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    338\u001b[0m         num_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100000\u001b[39m,\n\u001b[1;32m    339\u001b[0m         unroll_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m    340\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m    341\u001b[0m         learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m,\n\u001b[1;32m    342\u001b[0m         reward_cycle_period\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m    343\u001b[0m         fixed_seq_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m    344\u001b[0m         clip_eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[1;32m    345\u001b[0m         c1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m,\n\u001b[1;32m    346\u001b[0m         c2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m    347\u001b[0m         gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.99\u001b[39m,\n\u001b[1;32m    348\u001b[0m         lam\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m,\n\u001b[1;32m    349\u001b[0m         noptepochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m    350\u001b[0m         target_kl\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.02\u001b[39m\n\u001b[1;32m    351\u001b[0m     )\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 303\u001b[0m, in \u001b[0;36mtrain_recurrent_ppo\u001b[0;34m(env_name, num_timesteps, unroll_length, batch_size, learning_rate, reward_cycle_period, fixed_seq_length, clip_eps, c1, c2, gamma, lam, noptepochs, target_kl)\u001b[0m\n\u001b[1;32m    301\u001b[0m model \u001b[38;5;241m=\u001b[39m RecurrentActorCritic(action_dim\u001b[38;5;241m=\u001b[39maction_dim, hidden_dim\u001b[38;5;241m=\u001b[39mhidden_dim)\n\u001b[1;32m    302\u001b[0m key, init_key \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(key)\n\u001b[0;32m--> 303\u001b[0m carry \u001b[38;5;241m=\u001b[39m init_carry(batch_size, hidden_dim)\n\u001b[1;32m    304\u001b[0m params_model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39minit(init_key, init_obs, carry)\n\u001b[1;32m    305\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: params_model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction_dim\u001b[39m\u001b[38;5;124m'\u001b[39m: action_dim, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_dim\u001b[39m\u001b[38;5;124m'\u001b[39m: hidden_dim}\n",
      "Cell \u001b[0;32mIn[5], line 29\u001b[0m, in \u001b[0;36minit_carry\u001b[0;34m(batch_size, hidden_dim)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minit_carry\u001b[39m(batch_size, hidden_dim):\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m nn\u001b[38;5;241m.\u001b[39mLSTMCell\u001b[38;5;241m.\u001b[39minitialize_carry(jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mPRNGKey(\u001b[38;5;241m0\u001b[39m), (batch_size,), hidden_dim)\n",
      "File \u001b[0;32m~/.conda/envs/mujoco_env/lib/python3.12/site-packages/flax/linen/recurrent.py:187\u001b[0m, in \u001b[0;36mLSTMCell.initialize_carry\u001b[0;34m(self, rng, input_shape)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;129m@nowrap\u001b[39m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minitialize_carry\u001b[39m(\n\u001b[1;32m    177\u001b[0m   \u001b[38;5;28mself\u001b[39m, rng: PRNGKey, input_shape: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\n\u001b[1;32m    178\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[Array, Array]:\n\u001b[1;32m    179\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize the RNN cell carry.\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \n\u001b[1;32m    181\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;124;03m    An initialized carry for the given RNN cell.\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m   batch_dims \u001b[38;5;241m=\u001b[39m input_shape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    188\u001b[0m   key1, key2 \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msplit(rng)\n\u001b[1;32m    189\u001b[0m   mem_shape \u001b[38;5;241m=\u001b[39m batch_dims \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures,)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import flax.linen as nn\n",
    "import brax.envs as envs\n",
    "from functools import partial\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 1) Recurrent Actor-Critic Module\n",
    "# --------------------------------------------------------------------\n",
    "class RecurrentActorCritic(nn.Module):\n",
    "    action_dim: int\n",
    "    hidden_dim: int = 128\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, obs, carry):\n",
    "        # One-step LSTM cell.\n",
    "        lstm_cell = nn.LSTMCell()\n",
    "        new_carry, hidden = lstm_cell(carry, obs)\n",
    "        # Policy head.\n",
    "        mean = nn.Dense(self.action_dim)(hidden)\n",
    "        log_std = self.param('log_std', nn.initializers.zeros, (self.action_dim,))\n",
    "        # Value head.\n",
    "        value = nn.Dense(1)(hidden)\n",
    "        return mean, log_std, value, new_carry\n",
    "\n",
    "def init_carry(batch_size, hidden_dim):\n",
    "    return nn.LSTMCell.initialize_carry(jax.random.PRNGKey(0), (batch_size,), hidden_dim)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 2) Utility Functions\n",
    "# --------------------------------------------------------------------\n",
    "def sample_action(mean, log_std, key, deterministic=False):\n",
    "    std = jnp.exp(log_std)\n",
    "    if deterministic:\n",
    "        return mean\n",
    "    noise = jax.random.normal(key, shape=mean.shape)\n",
    "    return mean + std * noise\n",
    "\n",
    "def gaussian_log_prob(mean, log_std, action):\n",
    "    std = jnp.exp(log_std)\n",
    "    var = std ** 2\n",
    "    logp = -0.5 * (((action - mean) ** 2) / var + 2 * log_std + jnp.log(2 * jnp.pi))\n",
    "    return jnp.sum(logp, axis=-1)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 3) Reward Cycler (Optional)\n",
    "# --------------------------------------------------------------------\n",
    "class RewardCycler:\n",
    "    def __init__(self, period):\n",
    "        self.period = period\n",
    "        self.step_count = 0\n",
    "    def __call__(self, reward):\n",
    "        phase = (self.step_count // self.period) % 2\n",
    "        mod_reward = reward if phase == 0 else -reward\n",
    "        self.step_count += 1\n",
    "        return mod_reward\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 4) Rollout Generation\n",
    "# --------------------------------------------------------------------\n",
    "def rollout(params, env_state, carry, key, unroll_length, reward_cycler):\n",
    "    \"\"\"\n",
    "    Collect a rollout of length unroll_length from the environment.\n",
    "    Returns transitions (shape (T, B, ...)) and updated (carry, env_state, key).\n",
    "    \"\"\"\n",
    "    def step_fn(carry_env, _):\n",
    "        carry, state, key = carry_env\n",
    "        key, subkey = jax.random.split(key)\n",
    "        mean, log_std, value, new_carry = RecurrentActorCritic(\n",
    "            action_dim=params['action_dim'], hidden_dim=params['hidden_dim']\n",
    "        ).apply(params['model'], state.obs, carry)\n",
    "        action = sample_action(mean, log_std, subkey)\n",
    "        logp = gaussian_log_prob(mean, log_std, action)\n",
    "        next_state = state.step(action)\n",
    "        mod_reward = reward_cycler(next_state.reward)\n",
    "        done = next_state.done\n",
    "        # Reset LSTM state where done.\n",
    "        new_carry = jax.tree_util.tree_map(\n",
    "            lambda x: jnp.where(done[:, None], jnp.zeros_like(x), x),\n",
    "            new_carry\n",
    "        )\n",
    "        transition = {\n",
    "            'obs': state.obs,\n",
    "            'action': action,\n",
    "            'reward': mod_reward,\n",
    "            'value': value,\n",
    "            'done': done,\n",
    "            'logp': logp,\n",
    "        }\n",
    "        return (new_carry, next_state, key), transition\n",
    "\n",
    "    init = (carry, env_state, key)\n",
    "    (final_carry, final_env_state, final_key), transitions = jax.lax.scan(\n",
    "        step_fn, init, None, length=unroll_length\n",
    "    )\n",
    "    transitions = {k: jnp.stack([t[k] for t in transitions]) for k in transitions[0].keys()}\n",
    "    return transitions, final_carry, final_env_state, final_key\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 5) Trajectory Processing: Splitting, Padding, and Masking\n",
    "# --------------------------------------------------------------------\n",
    "def process_trajectories(transitions, fixed_seq_length):\n",
    "    \"\"\"\n",
    "    Splits each environment's trajectory (of length T) into fixed-length sequences.\n",
    "    Returns a dict with keys: 'obs', 'action', 'reward', 'value', 'logp', 'mask'\n",
    "    with shape (num_seq, fixed_seq_length, ...).\n",
    "    \"\"\"\n",
    "    T, B = transitions['reward'].shape[:2]\n",
    "    processed = {k: [] for k in ['obs', 'action', 'reward', 'value', 'logp']}\n",
    "    masks = []\n",
    "    for b in range(B):\n",
    "        traj = {k: transitions[k][:, b] for k in transitions}\n",
    "        done_indices = (traj['done'] == 1).nonzero()[0].tolist()\n",
    "        if not done_indices:\n",
    "            done_indices = [T - 1]\n",
    "        start = 0\n",
    "        for d in done_indices:\n",
    "            ep_len = d - start + 1\n",
    "            n_frags = (ep_len + fixed_seq_length - 1) // fixed_seq_length\n",
    "            for i in range(n_frags):\n",
    "                frag_start = start + i * fixed_seq_length\n",
    "                frag_end = min(frag_start + fixed_seq_length, start + ep_len)\n",
    "                frag = {}\n",
    "                for key in processed.keys():\n",
    "                    frag_data = traj[key][frag_start:frag_end]\n",
    "                    pad_len = fixed_seq_length - frag_data.shape[0]\n",
    "                    if pad_len > 0:\n",
    "                        pad_shape = (pad_len,) + frag_data.shape[1:]\n",
    "                        frag_data = jnp.concatenate([frag_data, jnp.zeros(pad_shape, dtype=frag_data.dtype)], axis=0)\n",
    "                    frag[key] = frag_data\n",
    "                for key in processed.keys():\n",
    "                    processed[key].append(frag[key])\n",
    "                valid = jnp.ones((frag_end - frag_start,), dtype=jnp.float32)\n",
    "                pad_mask = jnp.zeros((pad_len,), dtype=jnp.float32) if pad_len > 0 else jnp.array([])\n",
    "                mask = jnp.concatenate([valid, pad_mask])\n",
    "                masks.append(mask)\n",
    "            start = d + 1\n",
    "    for key in processed:\n",
    "        processed[key] = jnp.stack(processed[key], axis=0)\n",
    "    processed['mask'] = jnp.stack(masks, axis=0)\n",
    "    return processed\n",
    "\n",
    "def process_scalar_trajectories(scalar_array, done_array, fixed_seq_length):\n",
    "    \"\"\"\n",
    "    Process a scalar array (advantages or returns) with shape (T, B)\n",
    "    into shape (num_seq, fixed_seq_length) using similar logic.\n",
    "    \"\"\"\n",
    "    T, B = scalar_array.shape\n",
    "    result = []\n",
    "    for b in range(B):\n",
    "        arr = scalar_array[:, b]\n",
    "        dones = (done_array[:, b] == 1).nonzero()[0].tolist()\n",
    "        if not dones:\n",
    "            dones = [T - 1]\n",
    "        start = 0\n",
    "        for d in dones:\n",
    "            ep_len = d - start + 1\n",
    "            n_frags = (ep_len + fixed_seq_length - 1) // fixed_seq_length\n",
    "            for i in range(n_frags):\n",
    "                frag_start = start + i * fixed_seq_length\n",
    "                frag_end = min(frag_start + fixed_seq_length, start + ep_len)\n",
    "                frag_data = arr[frag_start:frag_end]\n",
    "                pad_len = fixed_seq_length - frag_data.shape[0]\n",
    "                if pad_len > 0:\n",
    "                    frag_data = jnp.concatenate([frag_data, jnp.zeros((pad_len,), dtype=frag_data.dtype)], axis=0)\n",
    "                result.append(frag_data)\n",
    "            start = d + 1\n",
    "    return jnp.stack(result, axis=0)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 6) Advantage & Return Computation\n",
    "# --------------------------------------------------------------------\n",
    "def normalize_advantages(advantages, eps=1e-8):\n",
    "    mean = jnp.mean(advantages)\n",
    "    std = jnp.std(advantages)\n",
    "    return (advantages - mean) / (std + eps)\n",
    "\n",
    "def compute_advantages(transitions, gamma=0.99, lam=0.95):\n",
    "    T, B = transitions['reward'].shape\n",
    "    rewards = transitions['reward']\n",
    "    values = transitions['value']\n",
    "    dones = transitions['done']\n",
    "    advantages = jnp.zeros((T, B))\n",
    "    returns = jnp.zeros((T, B))\n",
    "    last_advantage = jnp.zeros((B,))\n",
    "    def scan_fn(carry, t):\n",
    "        next_value, next_adv = carry\n",
    "        reward = rewards[t]\n",
    "        value = values[t]\n",
    "        mask = 1.0 - dones[t]\n",
    "        delta = reward + gamma * next_value * mask - value\n",
    "        adv = delta + gamma * lam * next_adv * mask\n",
    "        return (value, adv), adv\n",
    "    (_, adv_seq) = jax.lax.scan(scan_fn, (values[-1], last_advantage), jnp.arange(T-1, -1, -1))\n",
    "    advantages = adv_seq[::-1]\n",
    "    advantages = normalize_advantages(advantages)\n",
    "    returns = advantages + values\n",
    "    return advantages, returns\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 7) PPO Loss & KL Computation\n",
    "# --------------------------------------------------------------------\n",
    "def compute_loss(params, batch, advantages, returns, clip_eps, c1, c2):\n",
    "    \"\"\"\n",
    "    Compute PPO loss for a batch (a single sequence).\n",
    "    Returns total loss and auxiliary outputs (policy_loss, value_loss, new_logp_seq).\n",
    "    \"\"\"\n",
    "    def scan_step(carry, inp):\n",
    "        obs, act = inp\n",
    "        mean, log_std, val, new_carry = RecurrentActorCritic(\n",
    "            action_dim=params['action_dim'], hidden_dim=params['hidden_dim']\n",
    "        ).apply(params['model'], obs, carry)\n",
    "        new_logp = gaussian_log_prob(mean, log_std, act)\n",
    "        return new_carry, (new_logp, val)\n",
    "    init_carry_seq = init_carry(1, params['hidden_dim'])\n",
    "    new_logp_seq, val_seq = jax.lax.scan(\n",
    "        scan_step, init_carry_seq, (batch['obs'], batch['action'])\n",
    "    )[1]\n",
    "    new_logp_seq = new_logp_seq.squeeze(-1)  # shape (seq_len,)\n",
    "    val_seq = val_seq.squeeze(-1)            # shape (seq_len,)\n",
    "    old_logp_seq = batch['logp']              # shape (seq_len,)\n",
    "    mask = batch['mask']                     # shape (seq_len,)\n",
    "    ratio = jnp.exp(new_logp_seq - old_logp_seq)\n",
    "    surr1 = ratio * advantages\n",
    "    surr2 = jnp.clip(ratio, 1.0 - clip_eps, 1.0 + clip_eps) * advantages\n",
    "    policy_loss = -jnp.sum(jnp.minimum(surr1, surr2) * mask) / jnp.sum(mask)\n",
    "    value_loss = jnp.sum(((val_seq - returns) ** 2) * mask) / jnp.sum(mask)\n",
    "    entropy_bonus = 0.0  # For brevity.\n",
    "    total_loss = policy_loss + c1 * value_loss - c2 * entropy_bonus\n",
    "    return total_loss, (policy_loss, value_loss, new_logp_seq)\n",
    "\n",
    "def compute_kl(new_logp, old_logp, mask):\n",
    "    return jnp.sum((old_logp - new_logp) * mask) / jnp.sum(mask)\n",
    "\n",
    "def update_minibatch(params, opt_state, batch, adv, ret, optimizer, clip_eps, c1, c2, target_kl):\n",
    "    def loss_fn(p):\n",
    "        loss_val, aux = compute_loss(p, batch, adv, ret, clip_eps, c1, c2)\n",
    "        return loss_val, aux\n",
    "    (loss_val, (pol_loss, val_loss, new_logp_seq)), grads = jax.value_and_grad(loss_fn, has_aux=True)(params)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    kl_val = compute_kl(new_logp_seq, batch['logp'], batch['mask'])\n",
    "    return new_params, opt_state, kl_val, loss_val\n",
    "\n",
    "def update_policy(params, opt_state, processed, advantages, returns, optimizer,\n",
    "                  clip_eps, c1, c2, target_kl, noptepochs):\n",
    "    num_seq = processed['obs'].shape[0]\n",
    "    for epoch in range(noptepochs):\n",
    "        for i in range(num_seq):\n",
    "            batch = {k: processed[k][i] for k in ['obs', 'action', 'logp', 'mask']}\n",
    "            mb_adv = advantages[i]\n",
    "            mb_ret = returns[i]\n",
    "            params, opt_state, kl, loss_val = update_minibatch(params, opt_state, batch, mb_adv, mb_ret,\n",
    "                                                               optimizer, clip_eps, c1, c2, target_kl)\n",
    "            if kl > target_kl:\n",
    "                print(f\"Early stopping: KL {kl:.4f} exceeded target {target_kl} at epoch {epoch}, seq {i}\")\n",
    "                return params, opt_state\n",
    "    return params, opt_state\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 8) Main Training Loop\n",
    "# --------------------------------------------------------------------\n",
    "def train_recurrent_ppo(\n",
    "    env_name: str,\n",
    "    num_timesteps: int,\n",
    "    unroll_length: int,\n",
    "    batch_size: int,\n",
    "    learning_rate: float,\n",
    "    reward_cycle_period: int,\n",
    "    fixed_seq_length: int,\n",
    "    # PPO hyperparameters:\n",
    "    clip_eps=0.2,\n",
    "    c1=0.5,\n",
    "    c2=0.0,\n",
    "    gamma=0.99,\n",
    "    lam=0.95,\n",
    "    noptepochs=10,\n",
    "    target_kl=0.02\n",
    "):\n",
    "    \"\"\"\n",
    "    Main training loop:\n",
    "      1) Collect rollouts.\n",
    "      2) Process trajectories into fixed-length sequences.\n",
    "      3) Compute advantages and returns.\n",
    "      4) Update policy with multiple epochs and KL early stopping.\n",
    "    \"\"\"\n",
    "    env = envs.get_environment(env_name)\n",
    "    key = jax.random.PRNGKey(42)\n",
    "    key, reset_key = jax.random.split(key)\n",
    "    env_state = env.reset(reset_key)\n",
    "    \n",
    "    # Ensure observations are JAX arrays with a batch dimension.\n",
    "    init_obs = jnp.array(env_state.obs)\n",
    "    if init_obs.ndim == 1:\n",
    "        init_obs = init_obs[None, ...]\n",
    "    \n",
    "    action_dim = env.action_size\n",
    "    hidden_dim = 128\n",
    "    model = RecurrentActorCritic(action_dim=action_dim, hidden_dim=hidden_dim)\n",
    "    key, init_key = jax.random.split(key)\n",
    "    carry = init_carry(batch_size, hidden_dim)\n",
    "    params_model = model.init(init_key, init_obs, carry)\n",
    "    params = {'model': params_model, 'action_dim': action_dim, 'hidden_dim': hidden_dim}\n",
    "    \n",
    "    optimizer = optax.adam(learning_rate)\n",
    "    opt_state = optimizer.init(params)\n",
    "    \n",
    "    reward_cycler = RewardCycler(reward_cycle_period)\n",
    "    \n",
    "    total_updates = num_timesteps // unroll_length\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for t in range(total_updates):\n",
    "        key, rollout_key = jax.random.split(key)\n",
    "        transitions, new_carry, env_state, key = rollout(params, env_state, carry, rollout_key, unroll_length, reward_cycler)\n",
    "        carry = new_carry  # update carry for next rollout\n",
    "        \n",
    "        # Process trajectories: (num_seq, fixed_seq_length, ...)\n",
    "        processed = process_trajectories(transitions, fixed_seq_length)\n",
    "        adv_raw, ret_raw = compute_advantages(transitions, gamma, lam)\n",
    "        adv_proc = process_scalar_trajectories(adv_raw, transitions['done'], fixed_seq_length)\n",
    "        ret_proc = process_scalar_trajectories(ret_raw, transitions['done'], fixed_seq_length)\n",
    "        \n",
    "        params, opt_state = update_policy(\n",
    "            params, opt_state, processed, adv_proc, ret_proc,\n",
    "            optimizer, clip_eps, c1, c2, target_kl, noptepochs\n",
    "        )\n",
    "        if t % 100 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"Update {t}/{total_updates}, elapsed {elapsed:.2f}s\")\n",
    "    return params\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trained_params = train_recurrent_ppo(\n",
    "        env_name=\"humanoid\",\n",
    "        num_timesteps=100000,\n",
    "        unroll_length=20,\n",
    "        batch_size=32,\n",
    "        learning_rate=1e-4,\n",
    "        reward_cycle_period=100,\n",
    "        fixed_seq_length=10,\n",
    "        clip_eps=0.2,\n",
    "        c1=0.5,\n",
    "        c2=0.0,\n",
    "        gamma=0.99,\n",
    "        lam=0.95,\n",
    "        noptepochs=10,\n",
    "        target_kl=0.02\n",
    "    )\n",
    "    print(\"Training complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mujoco_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
