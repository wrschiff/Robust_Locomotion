{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mujoco in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (3.2.7)\n",
      "Requirement already satisfied: absl-py in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from mujoco) (2.1.0)\n",
      "Requirement already satisfied: etils[epath] in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from mujoco) (1.12.0)\n",
      "Requirement already satisfied: glfw in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from mujoco) (2.8.0)\n",
      "Requirement already satisfied: numpy in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from mujoco) (2.2.2)\n",
      "Requirement already satisfied: pyopengl in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from mujoco) (3.1.9)\n",
      "Requirement already satisfied: fsspec in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from etils[epath]->mujoco) (2025.2.0)\n",
      "Requirement already satisfied: importlib_resources in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from etils[epath]->mujoco) (6.5.2)\n",
      "Requirement already satisfied: typing_extensions in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from etils[epath]->mujoco) (4.12.2)\n",
      "Requirement already satisfied: zipp in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from etils[epath]->mujoco) (3.21.0)\n",
      "Requirement already satisfied: mujoco_mjx in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (3.2.7)\n",
      "Requirement already satisfied: absl-py in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from mujoco_mjx) (2.1.0)\n",
      "Requirement already satisfied: etils[epath] in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from mujoco_mjx) (1.12.0)\n",
      "Requirement already satisfied: jax in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from mujoco_mjx) (0.5.0)\n",
      "Requirement already satisfied: jaxlib in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from mujoco_mjx) (0.5.0)\n",
      "Requirement already satisfied: mujoco>=3.2.7.dev0 in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from mujoco_mjx) (3.2.7)\n",
      "Requirement already satisfied: scipy in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from mujoco_mjx) (1.15.1)\n",
      "Requirement already satisfied: trimesh in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from mujoco_mjx) (4.6.2)\n",
      "Requirement already satisfied: glfw in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from mujoco>=3.2.7.dev0->mujoco_mjx) (2.8.0)\n",
      "Requirement already satisfied: numpy in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from mujoco>=3.2.7.dev0->mujoco_mjx) (2.2.2)\n",
      "Requirement already satisfied: pyopengl in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from mujoco>=3.2.7.dev0->mujoco_mjx) (3.1.9)\n",
      "Requirement already satisfied: fsspec in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from etils[epath]->mujoco_mjx) (2025.2.0)\n",
      "Requirement already satisfied: importlib_resources in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from etils[epath]->mujoco_mjx) (6.5.2)\n",
      "Requirement already satisfied: typing_extensions in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from etils[epath]->mujoco_mjx) (4.12.2)\n",
      "Requirement already satisfied: zipp in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from etils[epath]->mujoco_mjx) (3.21.0)\n",
      "Requirement already satisfied: ml_dtypes>=0.4.0 in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from jax->mujoco_mjx) (0.5.1)\n",
      "Requirement already satisfied: opt_einsum in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from jax->mujoco_mjx) (3.4.0)\n",
      "Requirement already satisfied: brax in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (0.12.1)\n",
      "Requirement already satisfied: absl-py in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from brax) (2.1.0)\n",
      "Requirement already satisfied: dm_env in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from brax) (1.6)\n",
      "Requirement already satisfied: etils in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from brax) (1.12.0)\n",
      "Requirement already satisfied: flask in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from brax) (3.1.0)\n",
      "Requirement already satisfied: flask_cors in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from brax) (5.0.0)\n",
      "Requirement already satisfied: flax in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from brax) (0.10.3)\n",
      "Requirement already satisfied: grpcio in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from brax) (1.70.0)\n",
      "Requirement already satisfied: gym in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from brax) (0.26.2)\n",
      "Requirement already satisfied: jax>=0.4.6 in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from brax) (0.5.0)\n",
      "Requirement already satisfied: jaxlib>=0.4.6 in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from brax) (0.5.0)\n",
      "Requirement already satisfied: jaxopt in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from brax) (0.8.3)\n",
      "Requirement already satisfied: jinja2 in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from brax) (3.1.5)\n",
      "Requirement already satisfied: ml_collections in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from brax) (1.0.0)\n",
      "Requirement already satisfied: mujoco in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from brax) (3.2.7)\n",
      "Requirement already satisfied: mujoco-mjx in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from brax) (3.2.7)\n",
      "Requirement already satisfied: numpy in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from brax) (2.2.2)\n",
      "Requirement already satisfied: optax in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from brax) (0.2.4)\n",
      "Requirement already satisfied: orbax-checkpoint in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from brax) (0.11.5)\n",
      "Requirement already satisfied: Pillow in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from brax) (11.1.0)\n",
      "Requirement already satisfied: pytinyrenderer in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from brax) (0.0.14)\n",
      "Requirement already satisfied: scipy in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from brax) (1.15.1)\n",
      "Requirement already satisfied: tensorboardX in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from brax) (2.6.2.2)\n",
      "Requirement already satisfied: trimesh in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from brax) (4.6.2)\n",
      "Requirement already satisfied: typing-extensions in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from brax) (4.12.2)\n",
      "Requirement already satisfied: ml_dtypes>=0.4.0 in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from jax>=0.4.6->brax) (0.5.1)\n",
      "Requirement already satisfied: opt_einsum in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from jax>=0.4.6->brax) (3.4.0)\n",
      "Requirement already satisfied: dm-tree in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from dm_env->brax) (0.1.9)\n",
      "Requirement already satisfied: Werkzeug>=3.1 in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from flask->brax) (3.1.3)\n",
      "Requirement already satisfied: itsdangerous>=2.2 in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from flask->brax) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.3 in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from flask->brax) (8.1.8)\n",
      "Requirement already satisfied: blinker>=1.9 in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from flask->brax) (1.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from jinja2->brax) (3.0.2)\n",
      "Requirement already satisfied: msgpack in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from flax->brax) (1.1.0)\n",
      "Requirement already satisfied: tensorstore in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from flax->brax) (0.1.71)\n",
      "Requirement already satisfied: rich>=11.1 in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from flax->brax) (13.9.4)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from flax->brax) (6.0.2)\n",
      "Requirement already satisfied: treescope>=0.1.7 in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from flax->brax) (0.1.8)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from gym->brax) (3.1.1)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from gym->brax) (0.0.8)\n",
      "Requirement already satisfied: six in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from ml_collections->brax) (1.16.0)\n",
      "Requirement already satisfied: glfw in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from mujoco->brax) (2.8.0)\n",
      "Requirement already satisfied: pyopengl in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from mujoco->brax) (3.1.9)\n",
      "Requirement already satisfied: chex>=0.1.87 in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from optax->brax) (0.1.88)\n",
      "Requirement already satisfied: nest_asyncio in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from orbax-checkpoint->brax) (1.6.0)\n",
      "Requirement already satisfied: protobuf in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from orbax-checkpoint->brax) (5.29.3)\n",
      "Requirement already satisfied: humanize in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from orbax-checkpoint->brax) (4.11.0)\n",
      "Requirement already satisfied: simplejson>=3.16.0 in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from orbax-checkpoint->brax) (3.19.3)\n",
      "Requirement already satisfied: packaging in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from tensorboardX->brax) (24.2)\n",
      "Requirement already satisfied: setuptools in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from chex>=0.1.87->optax->brax) (75.8.0)\n",
      "Requirement already satisfied: toolz>=0.9.0 in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from chex>=0.1.87->optax->brax) (1.0.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from rich>=11.1->flax->brax) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from rich>=11.1->flax->brax) (2.15.1)\n",
      "Requirement already satisfied: attrs>=18.2.0 in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from dm-tree->dm_env->brax) (25.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.2 in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from dm-tree->dm_env->brax) (1.17.2)\n",
      "Requirement already satisfied: fsspec in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from etils[epath,epy]->orbax-checkpoint->brax) (2025.2.0)\n",
      "Requirement already satisfied: importlib_resources in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from etils[epath,epy]->orbax-checkpoint->brax) (6.5.2)\n",
      "Requirement already satisfied: zipp in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from etils[epath,epy]->orbax-checkpoint->brax) (3.21.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/wrschiff/.conda/envs/mujoco_env/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax->brax) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install mujoco\n",
    "!pip install mujoco_mjx\n",
    "!pip install brax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Mar  4 17:30:13 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A4000               On  |   00000000:65:00.0  On |                  Off |\n",
      "| 41%   50C    P5             23W /  140W |   13706MiB /  16376MiB |     24%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA RTX A4000               On  |   00000000:B3:00.0 Off |                  Off |\n",
      "| 41%   43C    P8             15W /  140W |   12349MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A     69934      G   /usr/lib/xorg/Xorg                            194MiB |\n",
      "|    0   N/A  N/A     70104      G   /usr/bin/gnome-shell                          169MiB |\n",
      "|    0   N/A  N/A     70486      G   ...erProcess --variations-seed-version         41MiB |\n",
      "|    0   N/A  N/A   1008287      G   ...sion,SpareRendererForSitePerProcess        328MiB |\n",
      "|    0   N/A  N/A   1028244      G   ...irefox/5783/usr/lib/firefox/firefox        219MiB |\n",
      "|    0   N/A  N/A   1062257      C   ...f/.conda/envs/mujoco_env/bin/python      12680MiB |\n",
      "|    1   N/A  N/A     69934      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    1   N/A  N/A   1062257      C   ...f/.conda/envs/mujoco_env/bin/python      12326MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "Environment variable MUJOCO_GL set to: egl\n",
      "Checking that the installation succeeded:\n",
      "Installation successful.\n",
      "XLA_FLAGS set to:  --xla_gpu_triton_gemm_any=True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import distutils.util\n",
    "\n",
    "# Check for GPU (remove or modify if you're running on a CPU-only machine)\n",
    "if subprocess.run('nvidia-smi', shell=True).returncode:\n",
    "    raise RuntimeError(\n",
    "        'Cannot communicate with GPU. '\n",
    "        'Make sure you have an NVIDIA GPU with the proper drivers installed.')\n",
    "\n",
    "# Add an ICD config so that glvnd can pick up the Nvidia EGL driver.\n",
    "NVIDIA_ICD_CONFIG_PATH = '/usr/share/glvnd/egl_vendor.d/10_nvidia.json'\n",
    "if not os.path.exists(NVIDIA_ICD_CONFIG_PATH):\n",
    "    with open(NVIDIA_ICD_CONFIG_PATH, 'w') as f:\n",
    "        f.write(\"\"\"{\n",
    "    \"file_format_version\" : \"1.0.0\",\n",
    "    \"ICD\" : {\n",
    "        \"library_path\" : \"libEGL_nvidia.so.0\"\n",
    "    }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "# Set environment variable to use GPU rendering:\n",
    "os.environ['MUJOCO_GL'] = 'egl'\n",
    "print('Environment variable MUJOCO_GL set to:', os.environ['MUJOCO_GL'])\n",
    "\n",
    "try:\n",
    "    print('Checking that the installation succeeded:')\n",
    "    import mujoco\n",
    "    mujoco.MjModel.from_xml_string('<mujoco/>')\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        'Something went wrong during installation. Check the terminal output for more information.\\n'\n",
    "        'If using a hosted runtime, make sure GPU acceleration is enabled.'\n",
    "    ) from e\n",
    "\n",
    "print('Installation successful.')\n",
    "\n",
    "# Tell XLA to use Triton GEMM for improved performance (if applicable)\n",
    "xla_flags = os.environ.get('XLA_FLAGS', '')\n",
    "xla_flags += ' --xla_gpu_triton_gemm_any=True'\n",
    "os.environ['XLA_FLAGS'] = xla_flags\n",
    "print('XLA_FLAGS set to:', os.environ['XLA_FLAGS'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All packages imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# If you haven't installed mediapy yet:\n",
    "# %pip install mediapy\n",
    "\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "from typing import Callable, NamedTuple, Optional, Union, List\n",
    "\n",
    "# Instead of the apt-install commands, just ensure ffmpeg is installed in your environment.\n",
    "\n",
    "import mediapy as media\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# More legible printing from numpy.\n",
    "np.set_printoptions(precision=3, suppress=True, linewidth=100)\n",
    "\n",
    "print(\"All packages imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Import MuJoCo, MJX, and Brax\n",
    "from datetime import datetime\n",
    "from etils import epath\n",
    "import functools\n",
    "from IPython.display import HTML\n",
    "from typing import Any, Dict, Sequence, Tuple, Union\n",
    "import os\n",
    "from ml_collections import config_dict\n",
    "\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jp\n",
    "import numpy as np\n",
    "from flax.training import orbax_utils\n",
    "from flax import struct\n",
    "from matplotlib import pyplot as plt\n",
    "import mediapy as media\n",
    "from orbax import checkpoint as ocp\n",
    "\n",
    "import mujoco\n",
    "from mujoco import mjx\n",
    "\n",
    "from brax import base\n",
    "from brax import envs\n",
    "from brax import math\n",
    "from brax.base import Base, Motion, Transform\n",
    "from brax.base import State as PipelineState\n",
    "from brax.envs.base import Env, PipelineEnv, State\n",
    "from brax.mjx.base import State as MjxState\n",
    "from brax.training.agents.ppo import train as ppo\n",
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "from brax.io import html, mjcf, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Humanoid Env\n",
    "#from pathlib import path\n",
    "import jax.numpy as jnp    \n",
    "\n",
    "#HUMANOID_ROOT_PATH = epath.Path(epath.resource_path('mujoco')) / 'mjx/test_data/humanoid'\n",
    "HUMANOID_ROOT_PATH = epath.Path(\"/home/wrschiff/Desktop/mujoco/Robust_Locomotion/\")\n",
    "\n",
    "class Humanoid(PipelineEnv):\n",
    "\n",
    "   def __init__(\n",
    "      self,\n",
    "      forward_reward_weight=1.25,\n",
    "      ctrl_cost_weight=0.1,\n",
    "      healthy_reward=5.0,\n",
    "      terminate_when_unhealthy=True,\n",
    "      healthy_z_range=(1.0, 2.0),\n",
    "      ########### WEIGHT FOR BALANCING TRAY ############\n",
    "      #balance_tray_reward_weight=0, # setting this to 0 for now\n",
    "      #terminate_when_boxfall=True,\n",
    "      ##################################################\n",
    "      #left_arm_pose_penalty_weight=0, #also setting this to 0 for initial policy\n",
    "      reset_noise_scale=1e-2,\n",
    "      exclude_current_positions_from_observation=True,\n",
    "      **kwargs,\n",
    "   ):\n",
    "   #\n",
    "      mj_model = mujoco.MjModel.from_xml_path(os.getcwd() + '/basic_humanoid_no_tray.xml')\n",
    "      mj_model.opt.solver = mujoco.mjtSolver.mjSOL_CG\n",
    "      mj_model.opt.iterations = 6\n",
    "      mj_model.opt.ls_iterations = 6\n",
    "\n",
    "      sys = mjcf.load_model(mj_model)\n",
    "\n",
    "      physics_steps_per_control_step = 5\n",
    "      kwargs['n_frames'] = kwargs.get(\n",
    "         'n_frames', physics_steps_per_control_step)\n",
    "      kwargs['backend'] = 'mjx'\n",
    "\n",
    "      super().__init__(sys, **kwargs)\n",
    "\n",
    "      self._forward_reward_weight = forward_reward_weight\n",
    "      self._ctrl_cost_weight = ctrl_cost_weight\n",
    "      self._healthy_reward = healthy_reward\n",
    "      self._terminate_when_unhealthy = terminate_when_unhealthy\n",
    "      self._healthy_z_range = healthy_z_range\n",
    "      self._reset_noise_scale = reset_noise_scale\n",
    "      ########### WEIGHT FOR BALANCING TRAY ############\n",
    "      #self._balance_tray_reward_weight = balance_tray_reward_weight\n",
    "      #self._terminate_when_boxfall = terminate_when_boxfall\n",
    "      ##################################################\n",
    "      self._exclude_current_positions_from_observation = (\n",
    "         exclude_current_positions_from_observation\n",
    "      )\n",
    "\n",
    "      ########### INDICES FOR BOX AND TRAY IN XPOS ARRAY ############\n",
    "      #self.tray_x_id = mujoco.mj_name2id(self.sys.mj_model, mujoco.mjtObj.mjOBJ_XBODY, \"tray\")\n",
    "      #self.box_x_id = mujoco.mj_name2id(self.sys.mj_model, mujoco.mjtObj.mjOBJ_XBODY, \"box\")\n",
    "      ###############################################################\n",
    "\n",
    "      #self._left_arm_pose_penalty_weight = left_arm_pose_penalty_weight\n",
    "\n",
    "      #self.target_left_arm_pose = jax.numpy.array([0.0, 0.0, 0.0])\n",
    "\n",
    "      # Identify left arm joint indices (adjust joint names as needed)\n",
    "      self.left_arm_joint_ids = jnp.array([\n",
    "         mujoco.mj_name2id(self.sys.mj_model, mujoco.mjtObj.mjOBJ_JOINT, \"shoulder1_left\"),\n",
    "         mujoco.mj_name2id(self.sys.mj_model, mujoco.mjtObj.mjOBJ_JOINT, \"shoulder2_left\"),\n",
    "         mujoco.mj_name2id(self.sys.mj_model, mujoco.mjtObj.mjOBJ_JOINT, \"elbow_left\")\n",
    "      ])\n",
    "\n",
    "\n",
    "   def reset(self, rng: jp.ndarray) -> State:\n",
    "      \"\"\"Resets the environment to an initial state.\"\"\"\n",
    "      rng, rng1, rng2 = jax.random.split(rng, 3)\n",
    "\n",
    "      low, hi = -self._reset_noise_scale, self._reset_noise_scale\n",
    "      qpos = self.sys.qpos0 + jax.random.uniform(\n",
    "         rng1, (self.sys.nq,), minval=low, maxval=hi\n",
    "      )\n",
    "      qvel = jax.random.uniform(\n",
    "         rng2, (self.sys.nv,), minval=low, maxval=hi\n",
    "      )\n",
    "\n",
    "      data = self.pipeline_init(qpos, qvel)\n",
    "\n",
    "      \n",
    "      obs = self._get_obs(data, jp.zeros(self.sys.nu))\n",
    "      reward, done, zero = jp.zeros(3)\n",
    "      metrics = {\n",
    "         'forward_reward': zero,\n",
    "         'reward_linvel': zero,\n",
    "         'reward_quadctrl': zero,\n",
    "         'reward_alive': zero,\n",
    "         'x_position': zero,\n",
    "         'y_position': zero,\n",
    "         'distance_from_origin': zero,\n",
    "         'x_velocity': zero,\n",
    "         'y_velocity': zero,\n",
    "      }\n",
    "      return State(data, obs, reward, done, metrics)\n",
    "   def step(self, state: State, action: jp.ndarray) -> State:\n",
    "      \"\"\"Runs one timestep of the environment's dynamics.\"\"\"\n",
    "      data0 = state.pipeline_state\n",
    "      data = self.pipeline_step(data0, action)\n",
    "\n",
    "      com_before = data0.subtree_com[1]\n",
    "      com_after = data.subtree_com[1]\n",
    "      velocity = (com_after - com_before) / self.dt\n",
    "      forward_reward = self._forward_reward_weight * velocity[0]\n",
    "\n",
    "      min_z, max_z = self._healthy_z_range\n",
    "      is_healthy = jp.where(data.q[2] < min_z, 0.0, 1.0)\n",
    "      is_healthy = jp.where(data.q[2] > max_z, 0.0, is_healthy)\n",
    "      if self._terminate_when_unhealthy:\n",
    "         healthy_reward = self._healthy_reward\n",
    "      else:\n",
    "         healthy_reward = self._healthy_reward * is_healthy\n",
    "\n",
    "      ctrl_cost = self._ctrl_cost_weight * jp.sum(jp.square(action))\n",
    "\n",
    "      ############## CALCULATE BOX-TRAY REWARD ##################\n",
    "      #euclid_dist_tb = jp.linalg.norm(data.x.pos[self.tray_x_id] - data.x.pos[self.box_x_id])\n",
    "      #balance_cost = euclid_dist_tb * self._balance_tray_reward_weight\n",
    "      ###########################################################\n",
    "\n",
    "      obs = self._get_obs(data, action)\n",
    "      ############## ADD TO OVERALL REWARD ##################\n",
    "      reward = forward_reward + healthy_reward - ctrl_cost - balance_cost\n",
    "      #######################################################\n",
    "\n",
    "      reward = forward_reward + healthy_reward - ctrl_cost - balance_cost\n",
    "\n",
    "      # Compute left arm steadiness penalty\n",
    "      #left_arm_joint_angles = data.q[self.left_arm_joint_ids]      \n",
    "      \n",
    "      # Compute pose deviation error\n",
    "      #pose_error = left_arm_joint_angles - self.target_left_arm_pose\n",
    "      #pose_penalty = self._left_arm_pose_penalty_weight * jp.sum(jp.square(pose_error))\n",
    "        \n",
    "       # Subtract the penalty from the total reward\n",
    "      #reward = reward - pose_penalty\n",
    "\n",
    "      #print(f'CTRL COST BEFORE SCALAR (as benchmark): {ctrl_cost}')\n",
    "      #print(f'EUCLID DISTANCE: {euclid_dist_tb} \\t\\tSCALED REWARD: {balance_cost}\\t\\tTOTAL REWARD: {reward}')\n",
    "\n",
    "      ########## ADDING TERMINATION CONSTRAINT IF BOX FALLS OFF TRAY ############\n",
    "      #is_balanced = data.x.pos[self.tray_x_id][2] < data.x.pos[self.box_x_id][2]\n",
    "      #done = 0.0\n",
    "      #done = jp.where(self._terminate_when_unhealthy, 1.0 - is_healthy, 0.0)\n",
    "      #done = jp.where(\n",
    "      #   (self._terminate_when_boxfall) & (done == 0.0),  # both must be True\n",
    "      #   1.0 - is_balanced,\n",
    "      #   done\n",
    "      #)\n",
    "\n",
    "      ###########################################################################\n",
    "      # PREVIOUS METHOD: done = 1.0 - is_healthy if self._terminate_when_unhealthy else 0.0\n",
    "\n",
    "      #print(f'TRAY HEIGHT: {data.x.pos[self.tray_x_id][2]}\\tBOX HEIGHT: {data.x.pos[self.box_x_id][2]}\\tDONE:{done}')\n",
    "      \n",
    "      state.metrics.update(\n",
    "         forward_reward=forward_reward,\n",
    "         reward_linvel=forward_reward,\n",
    "         reward_quadctrl=-ctrl_cost,\n",
    "         reward_alive=healthy_reward,\n",
    "         x_position=com_after[0],\n",
    "         y_position=com_after[1],\n",
    "         distance_from_origin=jp.linalg.norm(com_after),\n",
    "         x_velocity=velocity[0],\n",
    "         y_velocity=velocity[1],\n",
    "      )\n",
    "\n",
    "      return state.replace(\n",
    "         pipeline_state=data, obs=obs, reward=reward, done=done\n",
    "      )\n",
    "\n",
    "   def _get_obs(\n",
    "      self, data: mjx.Data, action: jp.ndarray\n",
    "   ) -> jp.ndarray:\n",
    "      \"\"\"Observes humanoid body position, velocities, and angles.\"\"\"\n",
    "      position = data.qpos\n",
    "      if self._exclude_current_positions_from_observation:\n",
    "         position = position[2:]\n",
    "\n",
    "      # external_contact_forces are excluded\n",
    "      return jp.concatenate([\n",
    "         position,\n",
    "         data.qvel,\n",
    "         data.cinert[1:].ravel(),\n",
    "         data.cvel[1:].ravel(),\n",
    "         data.qfrc_actuator,\n",
    "      ])\n",
    "envs.register_environment('humanoid', Humanoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the environment\n",
    "env_name = 'humanoid'\n",
    "env = envs.get_environment(env_name)\n",
    "\n",
    "# define the jit reset/step functions\n",
    "jit_reset = jax.jit(env.reset)\n",
    "jit_step = jax.jit(env.step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkWrapper:\n",
    "  \"\"\"A simple wrapper to emulate the interface expected by PPO.\n",
    "\n",
    "  It wraps a Flax model (a callable) along with a dummy input so that\n",
    "  the model's init and apply methods can be called without additional arguments.\n",
    "  \"\"\"\n",
    "  def __init__(self, model_fn, dummy_input):\n",
    "    self.model_fn = model_fn\n",
    "    self.dummy_input = dummy_input\n",
    "\n",
    "  def init(self, key):\n",
    "    return self.model_fn.init(key, self.dummy_input)\n",
    "\n",
    "  def apply(self, params, key, inputs):\n",
    "    return self.model_fn.apply(params, key, inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax.linen.recurrent import OptimizedLSTMCell\n",
    "from typing import Sequence, Tuple\n",
    "from types import SimpleNamespace\n",
    "from brax.training import distribution\n",
    "from brax.training import types\n",
    "\n",
    "class LSTMPolicy(nn.Module):\n",
    "    input_dim: int       # Dimensionality of a single observation.\n",
    "    hidden_dim: int      # Hidden size for the LSTM layers.\n",
    "    output_dim: int      # Number of action logits.\n",
    "    num_layers: int = 2  # Number of LSTM layers.\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, inputs, initial_state=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          inputs: jnp.ndarray of shape [batch, time, input_dim]\n",
    "          initial_state: Optional list of LSTM states for each layer.\n",
    "        Returns:\n",
    "          logits: jnp.ndarray of shape [batch, output_dim]\n",
    "          final_state: list of final LSTM states for each layer.\n",
    "        \"\"\"\n",
    "        batch_size, time_steps, _ = inputs.shape\n",
    "\n",
    "        # Initialize LSTM states if not provided.\n",
    "        if initial_state is None:\n",
    "            initial_state = [\n",
    "                OptimizedLSTMCell.initialize_carry(\n",
    "                    jax.random.PRNGKey(0),  # use a fixed key for initialization\n",
    "                    (batch_size,),\n",
    "                    self.hidden_dim\n",
    "                )\n",
    "                for _ in range(self.num_layers)\n",
    "            ]\n",
    "        state = initial_state\n",
    "\n",
    "        # Unroll the LSTM over the time dimension.\n",
    "        x = inputs  # shape: [batch, time, input_dim]\n",
    "        for t in range(time_steps):\n",
    "            xt = x[:, t, :]  # current timestep: [batch, input_dim] (or hidden_dim for subsequent layers)\n",
    "            new_state = []\n",
    "            h = xt\n",
    "            for i in range(self.num_layers):\n",
    "                # Each layer uses its own OptimizedLSTMCell.\n",
    "                cell = OptimizedLSTMCell(\n",
    "                    in_features=self.input_dim if i == 0 else self.hidden_dim,\n",
    "                    hidden_features=self.hidden_dim,\n",
    "                )\n",
    "                s, h = cell(state[i], h)\n",
    "                new_state.append(s)\n",
    "            state = new_state\n",
    "        final_output = h  # final hidden output from the last LSTM layer\n",
    "        logits = nn.Dense(self.output_dim)(final_output)\n",
    "        return logits, state\n",
    "\n",
    "# Example of how to wrap this into a network factory for PPO:\n",
    "\n",
    "def make_recurrent_ppo_networks(\n",
    "    obs_shape: types.ObservationSize,\n",
    "    action_size: int,\n",
    "    unroll_length: int = 20,\n",
    "    hidden_dim: int = 128,\n",
    "    num_layers: int = 2,\n",
    "    preprocess_observations_fn = lambda x: x,\n",
    ") -> SimpleNamespace:\n",
    "    \"\"\"\n",
    "    Returns a PPONetworks-like object with a recurrent LSTM-based policy network.\n",
    "    The value network remains a feedforward MLP.\n",
    "    \"\"\"\n",
    "    # Flatten the observation shape (e.g. (336,))\n",
    "    obs_dim = int(jnp.prod(jnp.array(obs_shape)))\n",
    "    \n",
    "    # Define the recurrent policy function.\n",
    "    def policy_fn(x):\n",
    "        # x is expected to be of shape [batch, unroll_length, obs_dim]\n",
    "        x = preprocess_observations_fn(x)\n",
    "        # Instantiate the LSTMPolicy module.\n",
    "        lstm_policy = LSTMPolicy(\n",
    "            input_dim=obs_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            output_dim=action_size,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        # We ignore the final LSTM state here.\n",
    "        logits, _ = lstm_policy(x)\n",
    "        return logits\n",
    "\n",
    "    # Transform the policy function so it can be initialized and applied.\n",
    "    # (You may need to adjust the wrapper to match Brax’s expectations.)\n",
    "    dummy_policy_input = jnp.zeros((1, unroll_length) + obs_shape)\n",
    "    policy_network = NetworkWrapper(policy_fn, dummy_policy_input)\n",
    "\n",
    "    # For the value network, we use a standard MLP.\n",
    "    class ValueMLP(nn.Module):\n",
    "        hidden_sizes: Sequence[int]\n",
    "        \n",
    "        @nn.compact\n",
    "        def __call__(self, x):\n",
    "            for size in self.hidden_sizes:\n",
    "                x = nn.Dense(size)(x)\n",
    "                x = nn.swish(x)\n",
    "            x = nn.Dense(1)(x)\n",
    "            return x\n",
    "\n",
    "    def value_fn(x):\n",
    "        # x is of shape [batch, obs_dim]\n",
    "        mlp = ValueMLP(hidden_sizes=[256, 256])\n",
    "        return mlp(x)\n",
    "    \n",
    "    dummy_value_input = jnp.zeros((1,) + obs_shape)\n",
    "    value_network = ppo_networks.NetworkWrapper(value_fn, dummy_value_input)\n",
    "    \n",
    "    # The parametric action distribution can remain the same.\n",
    "    parametric_action_distribution = distribution.NormalTanhDistribution(event_size=action_size)\n",
    "    \n",
    "    return SimpleNamespace(\n",
    "        policy_network=policy_network,\n",
    "        value_network=value_network,\n",
    "        parametric_action_distribution=parametric_action_distribution,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def domain_randomize(sys, rng):\n",
    "  \"\"\"Randomizes the mjx.Model.\"\"\"\n",
    "  @jax.vmap\n",
    "  def rand(rng):\n",
    "    _, key = jax.random.split(rng, 2)\n",
    "    # friction\n",
    "    friction = jax.random.uniform(key, (1,), minval=0.9, maxval=1.1)\n",
    "    friction = sys.geom_friction.at[:, 0].set(friction)\n",
    "    # actuator\n",
    "    _, key = jax.random.split(key, 2)\n",
    "    gain_range = (-2, 2)\n",
    "    param = jax.random.uniform(\n",
    "        key, (1,), minval=gain_range[0], maxval=gain_range[1]\n",
    "    ) + sys.actuator_gainprm[:, 0]\n",
    "    gain = sys.actuator_gainprm.at[:, 0].set(param)\n",
    "    bias = sys.actuator_biasprm.at[:, 1].set(-param)\n",
    "    return friction, gain, bias\n",
    "\n",
    "  friction, gain, bias = rand(rng)\n",
    "\n",
    "  in_axes = jax.tree_util.tree_map(lambda x: None, sys)\n",
    "  in_axes = in_axes.tree_replace({\n",
    "      'geom_friction': 0,\n",
    "      'actuator_gainprm': 0,\n",
    "      'actuator_biasprm': 0,\n",
    "  })\n",
    "\n",
    "  sys = sys.tree_replace({\n",
    "      'geom_friction': friction,\n",
    "      'actuator_gainprm': gain,\n",
    "      'actuator_biasprm': bias,\n",
    "  })\n",
    "\n",
    "  return sys, in_axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'brax.training.agents.ppo.networks' has no attribute 'NetworkWrapper'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 63\u001b[0m\n\u001b[1;32m     60\u001b[0m env \u001b[38;5;241m=\u001b[39m envs\u001b[38;5;241m.\u001b[39mget_environment(env_name)\n\u001b[1;32m     61\u001b[0m eval_env \u001b[38;5;241m=\u001b[39m envs\u001b[38;5;241m.\u001b[39mget_environment(env_name)\n\u001b[0;32m---> 63\u001b[0m make_inference_fn, params, _ \u001b[38;5;241m=\u001b[39m train_fn(\n\u001b[1;32m     64\u001b[0m     environment\u001b[38;5;241m=\u001b[39menv,\n\u001b[1;32m     65\u001b[0m     progress_fn\u001b[38;5;241m=\u001b[39mprogress,\n\u001b[1;32m     66\u001b[0m     eval_env\u001b[38;5;241m=\u001b[39meval_env\n\u001b[1;32m     67\u001b[0m )\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime to jit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimes[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mtimes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime to train: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimes[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mtimes[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/mujoco_env/lib/python3.12/site-packages/brax/training/agents/ppo/train.py:328\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(environment, num_timesteps, episode_length, wrap_env, wrap_env_fn, action_repeat, num_envs, max_devices_per_host, num_eval_envs, learning_rate, entropy_cost, discounting, seed, unroll_length, batch_size, num_minibatches, num_updates_per_batch, num_evals, num_resets_per_eval, normalize_observations, reward_scaling, clipping_epsilon, gae_lambda, deterministic_eval, network_factory, progress_fn, normalize_advantage, eval_env, policy_params_fn, randomization_fn, restore_checkpoint_path, max_grad_norm, madrona_backend, augment_pixels)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m normalize_observations:\n\u001b[1;32m    327\u001b[0m   normalize \u001b[38;5;241m=\u001b[39m running_statistics\u001b[38;5;241m.\u001b[39mnormalize\n\u001b[0;32m--> 328\u001b[0m ppo_network \u001b[38;5;241m=\u001b[39m network_factory(\n\u001b[1;32m    329\u001b[0m     obs_shape, env\u001b[38;5;241m.\u001b[39maction_size, preprocess_observations_fn\u001b[38;5;241m=\u001b[39mnormalize\n\u001b[1;32m    330\u001b[0m )\n\u001b[1;32m    331\u001b[0m make_policy \u001b[38;5;241m=\u001b[39m ppo_networks\u001b[38;5;241m.\u001b[39mmake_inference_fn(ppo_network)\n\u001b[1;32m    333\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optax\u001b[38;5;241m.\u001b[39madam(learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate)\n",
      "Cell \u001b[0;32mIn[44], line 114\u001b[0m, in \u001b[0;36mmake_recurrent_ppo_networks\u001b[0;34m(obs_shape, action_size, unroll_length, hidden_dim, num_layers, preprocess_observations_fn)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mlp(x)\n\u001b[1;32m    113\u001b[0m dummy_value_input \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m,) \u001b[38;5;241m+\u001b[39m obs_shape)\n\u001b[0;32m--> 114\u001b[0m value_network \u001b[38;5;241m=\u001b[39m ppo_networks\u001b[38;5;241m.\u001b[39mNetworkWrapper(value_fn, dummy_value_input)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# The parametric action distribution can remain the same.\u001b[39;00m\n\u001b[1;32m    117\u001b[0m parametric_action_distribution \u001b[38;5;241m=\u001b[39m distribution\u001b[38;5;241m.\u001b[39mNormalTanhDistribution(event_size\u001b[38;5;241m=\u001b[39maction_size)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'brax.training.agents.ppo.networks' has no attribute 'NetworkWrapper'"
     ]
    }
   ],
   "source": [
    "ckpt_path = epath.Path('/tmp/humanoid_base/ckpts')\n",
    "ckpt_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def policy_params_fn(current_step, make_policy, params):\n",
    "  # save checkpoints\n",
    "  orbax_checkpointer = ocp.PyTreeCheckpointer()\n",
    "  save_args = orbax_utils.save_args_from_target(params)\n",
    "  path = ckpt_path / f'{current_step}'\n",
    "  orbax_checkpointer.save(path, params, force=True, save_args=save_args)\n",
    "\n",
    "# Use your custom recurrent network factory.\n",
    "make_networks_factory = functools.partial(\n",
    "    make_recurrent_ppo_networks,  # This factory builds your LSTM-based policy.\n",
    "    hidden_dim=128,                # LSTM hidden state size.\n",
    "    num_layers=2                   # Number of LSTM layers.\n",
    ")\n",
    "\n",
    "train_fn = functools.partial(\n",
    "      ppo.train,\n",
    "      num_timesteps=100_000_000,\n",
    "      num_evals=10,\n",
    "      reward_scaling=1,\n",
    "      episode_length=1000,\n",
    "      normalize_observations=True,\n",
    "      action_repeat=1,\n",
    "      unroll_length=20,          # Unroll length for the recurrent policy.\n",
    "      num_minibatches=32,\n",
    "      num_updates_per_batch=4,\n",
    "      discounting=0.97,\n",
    "      learning_rate=3.0e-4,\n",
    "      entropy_cost=1e-2,\n",
    "      num_envs=8192,\n",
    "      batch_size=256,\n",
    "      network_factory=make_networks_factory,  # Pass the recurrent factory.\n",
    "      randomization_fn=domain_randomize,\n",
    "      policy_params_fn=policy_params_fn,\n",
    "      seed=0)\n",
    "\n",
    "x_data = []\n",
    "y_data = []\n",
    "ydataerr = []\n",
    "times = [datetime.now()]\n",
    "max_y, min_y = 1000, 0\n",
    "\n",
    "def progress(num_steps, metrics):\n",
    "  times.append(datetime.now())\n",
    "  x_data.append(num_steps)\n",
    "  y_data.append(metrics['eval/episode_reward'])\n",
    "  ydataerr.append(metrics['eval/episode_reward_std'])\n",
    "\n",
    "  plt.xlim([0, train_fn.keywords['num_timesteps'] * 1.25])\n",
    "  plt.ylim([min_y, max_y])\n",
    "  plt.xlabel('# environment steps')\n",
    "  plt.ylabel('reward per episode')\n",
    "  plt.title(f'y={y_data[-1]:.3f}')\n",
    "  plt.errorbar(x_data, y_data, yerr=ydataerr)\n",
    "  plt.show()\n",
    "\n",
    "# Reset environments (using Brax's environment API)\n",
    "env = envs.get_environment(env_name)\n",
    "eval_env = envs.get_environment(env_name)\n",
    "\n",
    "make_inference_fn, params, _ = train_fn(\n",
    "    environment=env,\n",
    "    progress_fn=progress,\n",
    "    eval_env=eval_env\n",
    ")\n",
    "\n",
    "print(f'time to jit: {times[1] - times[0]}')\n",
    "print(f'time to train: {times[-1] - times[1]}')\n",
    "\n",
    "# Save model parameters.\n",
    "model_path = '/home/wrschiff/Desktop/mujoco/Robust_Locomotion/model_paths/mjx_brax_initial_policy'\n",
    "model.save_params(model_path, params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mujoco_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
