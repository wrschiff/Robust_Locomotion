{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'pip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'pip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!pip install mujoco\n",
    "!pip install mujoco_mjx\n",
    "!pip install brax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/usr/share/glvnd/egl_vendor.d/10_nvidia.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m NVIDIA_ICD_CONFIG_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/usr/share/glvnd/egl_vendor.d/10_nvidia.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(NVIDIA_ICD_CONFIG_PATH):\n\u001b[1;32m---> 14\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mNVIDIA_ICD_CONFIG_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     15\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m{\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_format_version\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m : \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mICD\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m : \u001b[39m\u001b[38;5;124m{\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124m}\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Set environment variable to use GPU rendering:\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jjust\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/usr/share/glvnd/egl_vendor.d/10_nvidia.json'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import distutils.util\n",
    "\n",
    "# Check for GPU (remove or modify if you're running on a CPU-only machine)\n",
    "if subprocess.run('nvidia-smi', shell=True).returncode:\n",
    "    raise RuntimeError(\n",
    "        'Cannot communicate with GPU. '\n",
    "        'Make sure you have an NVIDIA GPU with the proper drivers installed.')\n",
    "\n",
    "# Add an ICD config so that glvnd can pick up the Nvidia EGL driver.\n",
    "NVIDIA_ICD_CONFIG_PATH = '/usr/share/glvnd/egl_vendor.d/10_nvidia.json'\n",
    "if not os.path.exists(NVIDIA_ICD_CONFIG_PATH):\n",
    "    with open(NVIDIA_ICD_CONFIG_PATH, 'w') as f:\n",
    "        f.write(\"\"\"{\n",
    "    \"file_format_version\" : \"1.0.0\",\n",
    "    \"ICD\" : {\n",
    "        \"library_path\" : \"libEGL_nvidia.so.0\"\n",
    "    }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "# Set environment variable to use GPU rendering:\n",
    "os.environ['MUJOCO_GL'] = 'egl'\n",
    "print('Environment variable MUJOCO_GL set to:', os.environ['MUJOCO_GL'])\n",
    "\n",
    "try:\n",
    "    print('Checking that the installation succeeded:')\n",
    "    import mujoco\n",
    "    mujoco.MjModel.from_xml_string('<mujoco/>')\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        'Something went wrong during installation. Check the terminal output for more information.\\n'\n",
    "        'If using a hosted runtime, make sure GPU acceleration is enabled.'\n",
    "    ) from e\n",
    "\n",
    "print('Installation successful.')\n",
    "\n",
    "# Tell XLA to use Triton GEMM for improved performance (if applicable)\n",
    "xla_flags = os.environ.get('XLA_FLAGS', '')\n",
    "xla_flags += ' --xla_gpu_triton_gemm_any=True'\n",
    "os.environ['XLA_FLAGS'] = xla_flags\n",
    "print('XLA_FLAGS set to:', os.environ['XLA_FLAGS'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All packages imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# If you haven't installed mediapy yet:\n",
    "# %pip install mediapy\n",
    "\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "from typing import Callable, NamedTuple, Optional, Union, List\n",
    "\n",
    "# Instead of the apt-install commands, just ensure ffmpeg is installed in your environment.\n",
    "\n",
    "import mediapy as media\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# More legible printing from numpy.\n",
    "np.set_printoptions(precision=3, suppress=True, linewidth=100)\n",
    "\n",
    "print(\"All packages imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Import MuJoCo, MJX, and Brax\n",
    "from datetime import datetime\n",
    "from etils import epath\n",
    "import functools\n",
    "from IPython.display import HTML\n",
    "from typing import Any, Dict, Sequence, Tuple, Union\n",
    "import os\n",
    "from ml_collections import config_dict\n",
    "\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jp\n",
    "import numpy as np\n",
    "from flax.training import orbax_utils\n",
    "from flax import struct\n",
    "from matplotlib import pyplot as plt\n",
    "import mediapy as media\n",
    "from orbax import checkpoint as ocp\n",
    "\n",
    "import mujoco\n",
    "from mujoco import mjx\n",
    "\n",
    "from brax import base\n",
    "from brax import envs\n",
    "from brax import math\n",
    "from brax.base import Base, Motion, Transform\n",
    "from brax.base import State as PipelineState\n",
    "from brax.envs.base import Env, PipelineEnv, State\n",
    "from brax.mjx.base import State as MjxState\n",
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "from brax.io import html, mjcf, model\n",
    "import ppo_train as ppo\n",
    "\n",
    "print(os.path.abspath(mujoco.__file__))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Humanoid Env\n",
    "#from pathlib import path\n",
    "import jax.numpy as jnp    \n",
    "\n",
    "#HUMANOID_ROOT_PATH = epath.Path(epath.resource_path('mujoco')) / 'mjx/test_data/humanoid'\n",
    "HUMANOID_ROOT_PATH = epath.Path(os.getcwd())\n",
    "\n",
    "class Humanoid(PipelineEnv):\n",
    "\n",
    "   def __init__(\n",
    "      self,\n",
    "      forward_reward_weight=1.25,\n",
    "      ctrl_cost_weight=0.1,\n",
    "      healthy_reward=5.0,\n",
    "      terminate_when_unhealthy=True,\n",
    "      healthy_z_range=(1.0, 2.0),\n",
    "      ########### WEIGHT FOR BALANCING TRAY ############\n",
    "      #balance_tray_reward_weight=0, # setting this to 0 for now\n",
    "      #terminate_when_boxfall=True,\n",
    "      ##################################################\n",
    "      #left_arm_pose_penalty_weight=0, #also setting this to 0 for initial policy\n",
    "      reset_noise_scale=1e-2,\n",
    "      exclude_current_positions_from_observation=True,\n",
    "      **kwargs,\n",
    "   ):\n",
    "   #\n",
    "      mj_model = mujoco.MjModel.from_xml_path(os.getcwd() + '/basic_humanoid_no_tray.xml')\n",
    "      mj_model.opt.solver = mujoco.mjtSolver.mjSOL_CG\n",
    "      mj_model.opt.iterations = 6\n",
    "      mj_model.opt.ls_iterations = 6\n",
    "\n",
    "      sys = mjcf.load_model(mj_model)\n",
    "\n",
    "      physics_steps_per_control_step = 5\n",
    "      kwargs['n_frames'] = kwargs.get(\n",
    "         'n_frames', physics_steps_per_control_step)\n",
    "      kwargs['backend'] = 'mjx'\n",
    "\n",
    "      super().__init__(sys, **kwargs)\n",
    "\n",
    "      self._forward_reward_weight = forward_reward_weight\n",
    "      self._ctrl_cost_weight = ctrl_cost_weight\n",
    "      self._healthy_reward = healthy_reward\n",
    "      self._terminate_when_unhealthy = terminate_when_unhealthy\n",
    "      self._healthy_z_range = healthy_z_range\n",
    "      self._reset_noise_scale = reset_noise_scale\n",
    "      ########### WEIGHT FOR BALANCING TRAY ############\n",
    "      #self._balance_tray_reward_weight = balance_tray_reward_weight\n",
    "      #self._terminate_when_boxfall = terminate_when_boxfall\n",
    "      ##################################################\n",
    "      self._exclude_current_positions_from_observation = (\n",
    "         exclude_current_positions_from_observation\n",
    "      )\n",
    "\n",
    "      ########### INDICES FOR BOX AND TRAY IN XPOS ARRAY ############\n",
    "      #self.tray_x_id = mujoco.mj_name2id(self.sys.mj_model, mujoco.mjtObj.mjOBJ_XBODY, \"tray\")\n",
    "      #self.box_x_id = mujoco.mj_name2id(self.sys.mj_model, mujoco.mjtObj.mjOBJ_XBODY, \"box\")\n",
    "      ###############################################################\n",
    "\n",
    "      #self._left_arm_pose_penalty_weight = left_arm_pose_penalty_weight\n",
    "\n",
    "      #self.target_left_arm_pose = jax.numpy.array([0.0, 0.0, 0.0])\n",
    "\n",
    "      # Identify left arm joint indices (adjust joint names as needed)\n",
    "      self.left_arm_joint_ids = jnp.array([\n",
    "         mujoco.mj_name2id(self.sys.mj_model, mujoco.mjtObj.mjOBJ_JOINT, \"shoulder1_left\"),\n",
    "         mujoco.mj_name2id(self.sys.mj_model, mujoco.mjtObj.mjOBJ_JOINT, \"shoulder2_left\"),\n",
    "         mujoco.mj_name2id(self.sys.mj_model, mujoco.mjtObj.mjOBJ_JOINT, \"elbow_left\")\n",
    "      ])\n",
    "\n",
    "\n",
    "   def reset(self, rng: jp.ndarray) -> State:\n",
    "      \"\"\"Resets the environment to an initial state.\"\"\"\n",
    "      rng, rng1, rng2 = jax.random.split(rng, 3)\n",
    "\n",
    "      low, hi = -self._reset_noise_scale, self._reset_noise_scale\n",
    "      qpos = self.sys.qpos0 + jax.random.uniform(\n",
    "         rng1, (self.sys.nq,), minval=low, maxval=hi\n",
    "      )\n",
    "      qvel = jax.random.uniform(\n",
    "         rng2, (self.sys.nv,), minval=low, maxval=hi\n",
    "      )\n",
    "\n",
    "      data = self.pipeline_init(qpos, qvel)\n",
    "\n",
    "      \n",
    "      obs = self._get_obs(data, jp.zeros(self.sys.nu))\n",
    "      reward, done, zero = jp.zeros(3)\n",
    "      metrics = {\n",
    "         'forward_reward': zero,\n",
    "         'reward_linvel': zero,\n",
    "         'reward_quadctrl': zero,\n",
    "         'reward_alive': zero,\n",
    "         'x_position': zero,\n",
    "         'y_position': zero,\n",
    "         'distance_from_origin': zero,\n",
    "         'x_velocity': zero,\n",
    "         'y_velocity': zero,\n",
    "      }\n",
    "      return State(data, obs, reward, done, metrics)\n",
    "   def step(self, state: State, action: jp.ndarray) -> State:\n",
    "      \"\"\"Runs one timestep of the environment's dynamics.\"\"\"\n",
    "      data0 = state.pipeline_state\n",
    "      data = self.pipeline_step(data0, action)\n",
    "\n",
    "      com_before = data0.subtree_com[1]\n",
    "      com_after = data.subtree_com[1]\n",
    "      velocity = (com_after - com_before) / self.dt\n",
    "      forward_reward = self._forward_reward_weight * velocity[0]\n",
    "\n",
    "      min_z, max_z = self._healthy_z_range\n",
    "      is_healthy = jp.where(data.q[2] < min_z, 0.0, 1.0)\n",
    "      is_healthy = jp.where(data.q[2] > max_z, 0.0, is_healthy)\n",
    "      if self._terminate_when_unhealthy:\n",
    "         healthy_reward = self._healthy_reward\n",
    "      else:\n",
    "         healthy_reward = self._healthy_reward * is_healthy\n",
    "\n",
    "      ctrl_cost = self._ctrl_cost_weight * jp.sum(jp.square(action))\n",
    "\n",
    "      ############## CALCULATE BOX-TRAY REWARD ##################\n",
    "      #euclid_dist_tb = jp.linalg.norm(data.x.pos[self.tray_x_id] - data.x.pos[self.box_x_id])\n",
    "      #balance_cost = euclid_dist_tb * self._balance_tray_reward_weight\n",
    "      ###########################################################\n",
    "\n",
    "      obs = self._get_obs(data, action)\n",
    "      ############## ADD TO OVERALL REWARD ##################\n",
    "      reward = forward_reward + healthy_reward - ctrl_cost - balance_cost\n",
    "      #######################################################\n",
    "\n",
    "      reward = forward_reward + healthy_reward - ctrl_cost - balance_cost\n",
    "\n",
    "      # Compute left arm steadiness penalty\n",
    "      #left_arm_joint_angles = data.q[self.left_arm_joint_ids]      \n",
    "      \n",
    "      # Compute pose deviation error\n",
    "      #pose_error = left_arm_joint_angles - self.target_left_arm_pose\n",
    "      #pose_penalty = self._left_arm_pose_penalty_weight * jp.sum(jp.square(pose_error))\n",
    "        \n",
    "       # Subtract the penalty from the total reward\n",
    "      #reward = reward - pose_penalty\n",
    "\n",
    "      #print(f'CTRL COST BEFORE SCALAR (as benchmark): {ctrl_cost}')\n",
    "      #print(f'EUCLID DISTANCE: {euclid_dist_tb} \\t\\tSCALED REWARD: {balance_cost}\\t\\tTOTAL REWARD: {reward}')\n",
    "\n",
    "      ########## ADDING TERMINATION CONSTRAINT IF BOX FALLS OFF TRAY ############\n",
    "      #is_balanced = data.x.pos[self.tray_x_id][2] < data.x.pos[self.box_x_id][2]\n",
    "      #done = 0.0\n",
    "      #done = jp.where(self._terminate_when_unhealthy, 1.0 - is_healthy, 0.0)\n",
    "      #done = jp.where(\n",
    "      #   (self._terminate_when_boxfall) & (done == 0.0),  # both must be True\n",
    "      #   1.0 - is_balanced,\n",
    "      #   done\n",
    "      #)\n",
    "\n",
    "      ###########################################################################\n",
    "      # PREVIOUS METHOD: done = 1.0 - is_healthy if self._terminate_when_unhealthy else 0.0\n",
    "\n",
    "      #print(f'TRAY HEIGHT: {data.x.pos[self.tray_x_id][2]}\\tBOX HEIGHT: {data.x.pos[self.box_x_id][2]}\\tDONE:{done}')\n",
    "      \n",
    "      state.metrics.update(\n",
    "         forward_reward=forward_reward,\n",
    "         reward_linvel=forward_reward,\n",
    "         reward_quadctrl=-ctrl_cost,\n",
    "         reward_alive=healthy_reward,\n",
    "         x_position=com_after[0],\n",
    "         y_position=com_after[1],\n",
    "         distance_from_origin=jp.linalg.norm(com_after),\n",
    "         x_velocity=velocity[0],\n",
    "         y_velocity=velocity[1],\n",
    "      )\n",
    "\n",
    "      return state.replace(\n",
    "         pipeline_state=data, obs=obs, reward=reward, done=done\n",
    "      )\n",
    "\n",
    "   def _get_obs(\n",
    "      self, data: mjx.Data, action: jp.ndarray\n",
    "   ) -> jp.ndarray:\n",
    "      \"\"\"Observes humanoid body position, velocities, and angles.\"\"\"\n",
    "      position = data.qpos\n",
    "      if self._exclude_current_positions_from_observation:\n",
    "         position = position[2:]\n",
    "\n",
    "      # external_contact_forces are excluded\n",
    "      return jp.concatenate([\n",
    "         position,\n",
    "         data.qvel,\n",
    "         data.cinert[1:].ravel(),\n",
    "         data.cvel[1:].ravel(),\n",
    "         data.qfrc_actuator,\n",
    "      ])\n",
    "envs.register_environment('humanoid', Humanoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the environment\n",
    "env_name = 'humanoid'\n",
    "env = envs.get_environment(env_name)\n",
    "\n",
    "# define the jit reset/step functions\n",
    "jit_reset = jax.jit(env.reset)\n",
    "jit_step = jax.jit(env.step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkWrapper:\n",
    "  \"\"\"A simple wrapper to emulate the interface expected by PPO.\n",
    "\n",
    "  It wraps a Flax model (a callable) along with a dummy input so that\n",
    "  the model's init and apply methods can be called without additional arguments.\n",
    "  \"\"\"\n",
    "  def __init__(self, model_fn, dummy_input):\n",
    "    self.model_fn = model_fn\n",
    "    self.dummy_input = dummy_input\n",
    "\n",
    "  def init(self, key):\n",
    "    return self.model_fn.init(key, self.dummy_input)\n",
    "\n",
    "  def apply(self, params, key, inputs):\n",
    "    return self.model_fn.apply(params, key, inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax.linen.recurrent import OptimizedLSTMCell\n",
    "from typing import Sequence, Tuple\n",
    "from types import SimpleNamespace\n",
    "from brax.training import distribution\n",
    "from brax.training import types\n",
    "\n",
    "class LSTMPolicy(nn.Module):\n",
    "    input_dim: int       # Dimensionality of a single observation.\n",
    "    hidden_dim: int      # Hidden size for the LSTM layers.\n",
    "    output_dim: int      # Number of action logits.\n",
    "    num_layers: int = 2  # Number of LSTM layers.\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, inputs, initial_state=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          inputs: jnp.ndarray of shape [batch, time, input_dim]\n",
    "          initial_state: Optional list of LSTM states for each layer.\n",
    "        Returns:\n",
    "          logits: jnp.ndarray of shape [batch, output_dim]\n",
    "          final_state: list of final LSTM states for each layer.\n",
    "        \"\"\"\n",
    "        batch_size, time_steps, _ = inputs.shape\n",
    "\n",
    "        # Initialize LSTM states if not provided.\n",
    "        if initial_state is None:\n",
    "            initial_state = [\n",
    "                OptimizedLSTMCell.initialize_carry(\n",
    "                    jax.random.PRNGKey(0),  # use a fixed key for initialization\n",
    "                    (batch_size,),\n",
    "                    self.hidden_dim\n",
    "                )\n",
    "                for _ in range(self.num_layers)\n",
    "            ]\n",
    "        state = initial_state\n",
    "\n",
    "        # Unroll the LSTM over the time dimension.\n",
    "        x = inputs  # shape: [batch, time, input_dim]\n",
    "        for t in range(time_steps):\n",
    "            xt = x[:, t, :]  # current timestep: [batch, input_dim] (or hidden_dim for subsequent layers)\n",
    "            new_state = []\n",
    "            h = xt\n",
    "            for i in range(self.num_layers):\n",
    "                # Each layer uses its own OptimizedLSTMCell.\n",
    "                cell = OptimizedLSTMCell(\n",
    "                    in_features=self.input_dim if i == 0 else self.hidden_dim,\n",
    "                    hidden_features=self.hidden_dim,\n",
    "                )\n",
    "                s, h = cell(state[i], h)\n",
    "                new_state.append(s)\n",
    "            state = new_state\n",
    "        final_output = h  # final hidden output from the last LSTM layer\n",
    "        logits = nn.Dense(self.output_dim)(final_output)\n",
    "        return logits, state\n",
    "\n",
    "# Example of how to wrap this into a network factory for PPO:\n",
    "\n",
    "def make_recurrent_ppo_networks(\n",
    "    obs_shape: types.ObservationSize,\n",
    "    action_size: int,\n",
    "    unroll_length: int = 20,\n",
    "    hidden_dim: int = 128,\n",
    "    num_layers: int = 2,\n",
    "    preprocess_observations_fn = lambda x: x,\n",
    ") -> SimpleNamespace:\n",
    "    \"\"\"\n",
    "    Returns a PPONetworks-like object with a recurrent LSTM-based policy network.\n",
    "    The value network remains a feedforward MLP.\n",
    "    \"\"\"\n",
    "    # Flatten the observation shape (e.g. (336,))\n",
    "    obs_dim = int(jnp.prod(jnp.array(obs_shape)))\n",
    "    \n",
    "    # Define the recurrent policy function.\n",
    "    def policy_fn(x):\n",
    "        # x is expected to be of shape [batch, unroll_length, obs_dim]\n",
    "        x = preprocess_observations_fn(x)\n",
    "        # Instantiate the LSTMPolicy module.\n",
    "        lstm_policy = LSTMPolicy(\n",
    "            input_dim=obs_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            output_dim=action_size,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        # We ignore the final LSTM state here.\n",
    "        logits, _ = lstm_policy(x)\n",
    "        return logits\n",
    "\n",
    "    # Transform the policy function so it can be initialized and applied.\n",
    "    # (You may need to adjust the wrapper to match Brax’s expectations.)\n",
    "    dummy_policy_input = jnp.zeros((1, unroll_length) + obs_shape)\n",
    "    policy_network = NetworkWrapper(policy_fn, dummy_policy_input)\n",
    "\n",
    "    # For the value network, we use a standard MLP.\n",
    "    class ValueMLP(nn.Module):\n",
    "        hidden_sizes: Sequence[int]\n",
    "        \n",
    "        @nn.compact\n",
    "        def __call__(self, x):\n",
    "            for size in self.hidden_sizes:\n",
    "                x = nn.Dense(size)(x)\n",
    "                x = nn.swish(x)\n",
    "            x = nn.Dense(1)(x)\n",
    "            return x\n",
    "\n",
    "    def value_fn(x):\n",
    "        # x is of shape [batch, obs_dim]\n",
    "        mlp = ValueMLP(hidden_sizes=[256, 256])\n",
    "        return mlp(x)\n",
    "    \n",
    "    dummy_value_input = jnp.zeros((1,) + obs_shape)\n",
    "    value_network = NetworkWrapper(value_fn, dummy_value_input)\n",
    "    \n",
    "    # The parametric action distribution can remain the same.\n",
    "    parametric_action_distribution = distribution.NormalTanhDistribution(event_size=action_size)\n",
    "    \n",
    "    return SimpleNamespace(\n",
    "        policy_network=policy_network,\n",
    "        value_network=value_network,\n",
    "        parametric_action_distribution=parametric_action_distribution,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def domain_randomize(sys, rng):\n",
    "  \"\"\"Randomizes the mjx.Model.\"\"\"\n",
    "  @jax.vmap\n",
    "  def rand(rng):\n",
    "    _, key = jax.random.split(rng, 2)\n",
    "    # friction\n",
    "    friction = jax.random.uniform(key, (1,), minval=0.9, maxval=1.1)\n",
    "    friction = sys.geom_friction.at[:, 0].set(friction)\n",
    "    # actuator\n",
    "    _, key = jax.random.split(key, 2)\n",
    "    gain_range = (-2, 2)\n",
    "    param = jax.random.uniform(\n",
    "        key, (1,), minval=gain_range[0], maxval=gain_range[1]\n",
    "    ) + sys.actuator_gainprm[:, 0]\n",
    "    gain = sys.actuator_gainprm.at[:, 0].set(param)\n",
    "    bias = sys.actuator_biasprm.at[:, 1].set(-param)\n",
    "    return friction, gain, bias\n",
    "\n",
    "  friction, gain, bias = rand(rng)\n",
    "\n",
    "  in_axes = jax.tree_util.tree_map(lambda x: None, sys)\n",
    "  in_axes = in_axes.tree_replace({\n",
    "      'geom_friction': 0,\n",
    "      'actuator_gainprm': 0,\n",
    "      'actuator_biasprm': 0,\n",
    "  })\n",
    "\n",
    "  sys = sys.tree_replace({\n",
    "      'geom_friction': friction,\n",
    "      'actuator_gainprm': gain,\n",
    "      'actuator_biasprm': bias,\n",
    "  })\n",
    "\n",
    "  return sys, in_axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'init'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 63\u001b[0m\n\u001b[0;32m     60\u001b[0m env \u001b[38;5;241m=\u001b[39m envs\u001b[38;5;241m.\u001b[39mget_environment(env_name)\n\u001b[0;32m     61\u001b[0m eval_env \u001b[38;5;241m=\u001b[39m envs\u001b[38;5;241m.\u001b[39mget_environment(env_name)\n\u001b[1;32m---> 63\u001b[0m make_inference_fn, params, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43menvironment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprogress_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_env\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime to jit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimes[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mtimes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime to train: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimes[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mtimes[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jjust\\Proj\\school\\291I\\Robust_Locomotion\\ppo_train.py:600\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(environment, num_timesteps, max_devices_per_host, wrap_env, madrona_backend, augment_pixels, num_envs, episode_length, action_repeat, wrap_env_fn, randomization_fn, learning_rate, entropy_cost, discounting, unroll_length, batch_size, num_minibatches, num_updates_per_batch, num_resets_per_eval, normalize_observations, reward_scaling, clipping_epsilon, gae_lambda, max_grad_norm, normalize_advantage, network_factory, seed, num_evals, eval_env, num_eval_envs, deterministic_eval, log_training_metrics, training_metrics_steps, progress_fn, policy_params_fn, save_checkpoint_path, restore_checkpoint_path, restore_params, restore_value_fn)\u001b[0m\n\u001b[0;32m    596\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m training_state, env_state, metrics  \u001b[38;5;66;03m# pytype: disable=bad-return-type  # py311-upgrade\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;66;03m# Initialize model params and training state.\u001b[39;00m\n\u001b[0;32m    599\u001b[0m init_params \u001b[38;5;241m=\u001b[39m ppo_losses\u001b[38;5;241m.\u001b[39mPPONetworkParams(\n\u001b[1;32m--> 600\u001b[0m     policy\u001b[38;5;241m=\u001b[39m\u001b[43mppo_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_policy\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    601\u001b[0m     value\u001b[38;5;241m=\u001b[39mppo_network\u001b[38;5;241m.\u001b[39mvalue_network\u001b[38;5;241m.\u001b[39minit(key_value),\n\u001b[0;32m    602\u001b[0m )\n\u001b[0;32m    604\u001b[0m obs_shape \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mtree_util\u001b[38;5;241m.\u001b[39mtree_map(\n\u001b[0;32m    605\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: specs\u001b[38;5;241m.\u001b[39mArray(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:], jnp\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)), env_state\u001b[38;5;241m.\u001b[39mobs\n\u001b[0;32m    606\u001b[0m )\n\u001b[0;32m    607\u001b[0m training_state \u001b[38;5;241m=\u001b[39m TrainingState(  \u001b[38;5;66;03m# pytype: disable=wrong-arg-types  # jax-ndarray\u001b[39;00m\n\u001b[0;32m    608\u001b[0m     optimizer_state\u001b[38;5;241m=\u001b[39moptimizer\u001b[38;5;241m.\u001b[39minit(init_params),  \u001b[38;5;66;03m# pytype: disable=wrong-arg-types  # numpy-scalars\u001b[39;00m\n\u001b[0;32m    609\u001b[0m     params\u001b[38;5;241m=\u001b[39minit_params,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    613\u001b[0m     env_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    614\u001b[0m )\n",
      "Cell \u001b[1;32mIn[8], line 12\u001b[0m, in \u001b[0;36mNetworkWrapper.init\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minit\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m---> 12\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdummy_input)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'init'"
     ]
    }
   ],
   "source": [
    "ckpt_path = epath.Path('/tmp/humanoid_base/ckpts')\n",
    "ckpt_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def policy_params_fn(current_step, make_policy, params):\n",
    "  # save checkpoints\n",
    "  orbax_checkpointer = ocp.PyTreeCheckpointer()\n",
    "  save_args = orbax_utils.save_args_from_target(params)\n",
    "  path = ckpt_path / f'{current_step}'\n",
    "  orbax_checkpointer.save(path, params, force=True, save_args=save_args)\n",
    "\n",
    "# Use your custom recurrent network factory.\n",
    "make_networks_factory = functools.partial(\n",
    "    make_recurrent_ppo_networks,  # This factory builds your LSTM-based policy.\n",
    "    hidden_dim=128,                # LSTM hidden state size.\n",
    "    num_layers=2                   # Number of LSTM layers.\n",
    ")\n",
    "\n",
    "train_fn = functools.partial(\n",
    "      ppo.train,\n",
    "      num_timesteps=100_000_000,\n",
    "      num_evals=10,\n",
    "      reward_scaling=1,\n",
    "      episode_length=1000,\n",
    "      normalize_observations=True,\n",
    "      action_repeat=1,\n",
    "      unroll_length=20,          # Unroll length for the recurrent policy.\n",
    "      num_minibatches=32,\n",
    "      num_updates_per_batch=4,\n",
    "      discounting=0.97,\n",
    "      learning_rate=3.0e-4,\n",
    "      entropy_cost=1e-2,\n",
    "      num_envs=8192,\n",
    "      batch_size=256,\n",
    "      network_factory=make_networks_factory,  # Pass the recurrent factory.\n",
    "      randomization_fn=domain_randomize,\n",
    "      policy_params_fn=policy_params_fn,\n",
    "      seed=0)\n",
    "\n",
    "x_data = []\n",
    "y_data = []\n",
    "ydataerr = []\n",
    "times = [datetime.now()]\n",
    "max_y, min_y = 1000, 0\n",
    "\n",
    "def progress(num_steps, metrics):\n",
    "  times.append(datetime.now())\n",
    "  x_data.append(num_steps)\n",
    "  y_data.append(metrics['eval/episode_reward'])\n",
    "  ydataerr.append(metrics['eval/episode_reward_std'])\n",
    "\n",
    "  plt.xlim([0, train_fn.keywords['num_timesteps'] * 1.25])\n",
    "  plt.ylim([min_y, max_y])\n",
    "  plt.xlabel('# environment steps')\n",
    "  plt.ylabel('reward per episode')\n",
    "  plt.title(f'y={y_data[-1]:.3f}')\n",
    "  plt.errorbar(x_data, y_data, yerr=ydataerr)\n",
    "  plt.show()\n",
    "\n",
    "# Reset environments (using Brax's environment API)\n",
    "env = envs.get_environment(env_name)\n",
    "eval_env = envs.get_environment(env_name)\n",
    "\n",
    "make_inference_fn, params, _ = train_fn(\n",
    "    environment=env,\n",
    "    progress_fn=progress,\n",
    "    eval_env=eval_env\n",
    ")\n",
    "\n",
    "print(f'time to jit: {times[1] - times[0]}')\n",
    "print(f'time to train: {times[-1] - times[1]}')\n",
    "\n",
    "# Save model parameters.\n",
    "model_path = f'{os.getcwd()}/mjx_brax_initial_policy'\n",
    "model.save_params(model_path, params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
